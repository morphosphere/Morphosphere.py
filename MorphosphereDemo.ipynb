{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.\n"
     ]
    }
   ],
   "source": [
    "import cPickle as pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob,os\n",
    "import cv2\n",
    "\n",
    "#theano\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import timeit\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline  \n",
    "\n",
    "from segmentSpheroid import segmentSpheroid\n",
    "from generateBatch import generateBatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N:\\Vardan_Andriasyan\\Morphosphere\\2classes\\\\Test\\NonSpheroid\\20160126-corning-all-spheroids-p4-006hps_D05_w1.TIF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "segmentSpheroid.py:55: VisibleDeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future\n",
      "  selectedCC = (labeledImage == allProperties[indexOfMinDistance].label)\n",
      "segmentSpheroid.py:59: VisibleDeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future\n",
      "  boundingBox =  allProperties[indexOfMinDistance].bbox\n",
      "segmentSpheroid.py:62: VisibleDeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future\n",
      "  centroid = allProperties[indexOfMinDistance].centroid\n",
      "segmentSpheroid.py:63: VisibleDeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future\n",
      "  perimeter = allProperties[indexOfMinDistance].perimeter\n",
      "segmentSpheroid.py:64: VisibleDeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future\n",
      "  area = allProperties[indexOfMinDistance].area\n",
      "segmentSpheroid.py:65: VisibleDeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future\n",
      "  diameter = allProperties[indexOfMinDistance].equivalent_diameter\n",
      "segmentSpheroid.py:66: VisibleDeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future\n",
      "  majorAxis = allProperties[indexOfMinDistance].major_axis_length\n",
      "segmentSpheroid.py:67: VisibleDeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future\n",
      "  minorAxis = allProperties[indexOfMinDistance].minor_axis_length\n",
      "segmentSpheroid.py:79: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  croppedBWImage = spheroidBWImage[minRow:maxRow,minCol:maxCol]\n",
      "segmentSpheroid.py:80: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  croppedImage  =  inputImage[minRow:maxRow,minCol:maxCol]*croppedBWImage\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N:\\Vardan_Andriasyan\\Morphosphere\\2classes\\\\Test\\NonSpheroid\\20160126-corning-all-spheroids-p4-006hps_F05_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\2classes\\\\Test\\NonSpheroid\\20160126-corning-all-spheroids-p4-006hps_I03_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\2classes\\\\Test\\NonSpheroid\\20160126-corning-all-spheroids-p5-007hps_M01_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\2classes\\\\Test\\NonSpheroid\\20160127-corning-all-spheroids-p1-020hps_B05_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\2classes\\\\Test\\Spheroid\\20160130-corning-all-spheroids-p5-098hps_F06_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\2classes\\\\Test\\Spheroid\\20160130-corning-all-spheroids-p2-095hps_O05_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\2classes\\\\Test\\Spheroid\\20160130-corning-all-spheroids-p5-098hps_E01_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\2classes\\\\Test\\Spheroid\\20160130-corning-all-spheroids-p5-098hps_E06_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\2classes\\\\Test\\Spheroid\\20160130-corning-all-spheroids-p5-098hps_F01_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\2classes\\\\Training\\NonSpheroid\\20160126-corning-all-spheroids-p4-006hps_E02_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\2classes\\\\Training\\NonSpheroid\\20160126-corning-all-spheroids-p4-006hps_G05_w1.TIF\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-c096f860e7ff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrentImagePath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[1;31m#current ImagePath, diskSize, blocksize for threshold, min clump area [px]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m             \u001b[0mcroppedImage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcroppedBWImage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspheroidAttributes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msegmentSpheroid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrentImagePath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m501\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m             \u001b[1;31m#populate an array with all image sizes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[0mimageHeight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimageWidth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcroppedImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\FannyG\\Documents\\GitHub\\Morphosphere.py\\segmentSpheroid.pyc\u001b[0m in \u001b[0;36msegmentSpheroid\u001b[1;34m(inputImagePath, dilationDisk, blockSize, minSpheroidArea)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mthresholdedImage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mndimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary_fill_holes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthresholdedImage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0mlabeledImage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeasure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthresholdedImage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m     \u001b[0mallProperties\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeasure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregionprops\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabeledImage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\FannyG\\Documents\\Anaconda-2\\lib\\site-packages\\skimage\\measure\\_label.pyc\u001b[0m in \u001b[0;36mlabel\u001b[1;34m(input, neighbors, background, return_num, connectivity)\u001b[0m\n\u001b[0;32m      3\u001b[0m def label(input, neighbors=None, background=None, return_num=False,\n\u001b[0;32m      4\u001b[0m           connectivity=None):\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneighbors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackground\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_num\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconnectivity\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_label\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mskimage/measure/_ccomp.pyx\u001b[0m in \u001b[0;36mskimage.measure._ccomp.label (skimage\\measure\\_ccomp.c:3889)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mskimage/measure/_ccomp.pyx\u001b[0m in \u001b[0;36mskimage.measure._ccomp._label (skimage\\measure\\_ccomp.c:4280)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\FannyG\\Documents\\Anaconda-2\\lib\\site-packages\\numpy\\lib\\function_base.pyc\u001b[0m in \u001b[0;36mcopy\u001b[1;34m(a, order)\u001b[0m\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1307\u001b[0m     \"\"\"\n\u001b[1;32m-> 1308\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1309\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1310\u001b[0m \u001b[1;31m# Basic operations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "##############################################\n",
    "#################2 CLASSES####################\n",
    "##############################################\n",
    "\"\"\"\n",
    "Demo for Morphosphere\n",
    "\n",
    "@author: Vardan Andriasyan\n",
    "\"\"\"\n",
    "\n",
    "#\n",
    "parentDir = 'N:\\\\Vardan_Andriasyan\\\\Morphosphere\\\\2classes\\\\'\n",
    "sets = ['Test','Training','Validation']#'Test','Training','Validation'\n",
    "classes = ['NonSpheroid','Spheroid']#'NonSphHealthy','NonSphNonHealthy','SphHealthy','SphNonHealthy'\n",
    "\n",
    "#inputImagePath= 'N:\\\\Vardan_Andriasyan\\\\Morphosphere\\\\HT-29\\\\Test\\\\NonSphNonHealthy\\\\20160205-corning-all-spheroids-p4-242hps_G05_w1.TIF'\n",
    "#t = cv2.imread(inputImagePath,0)\n",
    "#print(t)\n",
    "\n",
    "\n",
    "#fullImage, croppedImage, croppedBWImage, spheroidAttributes = segmentSpheroid(inputImage, 12, 501, 20000)\n",
    "\n",
    "trainingSpheroids = []\n",
    "testSpheroids = []\n",
    "validationSpheroids = []\n",
    "trainingLabels = []\n",
    "testLabels = []\n",
    "validationLabels = []\n",
    "\n",
    "allSizes = []\n",
    "\n",
    "for iSet in sets:\n",
    "    for iClass in classes:\n",
    "        inputFolderPath = parentDir+'\\\\'+iSet+'\\\\'+iClass\n",
    "        for currentImagePath in glob.glob(inputFolderPath+'\\\\*.TIF'):\n",
    "            print(currentImagePath)\n",
    "            #current ImagePath, diskSize, blocksize for threshold, min clump area [px]\n",
    "            croppedImage, croppedBWImage, spheroidAttributes = segmentSpheroid(currentImagePath, 20, 501, 20000)\n",
    "            #populate an array with all image sizes\n",
    "            imageHeight, imageWidth = croppedImage.shape[:2]\n",
    "            allSizes.append(max([imageHeight,imageWidth]))\n",
    "            if iSet == 'Test':\n",
    "                testSpheroids.append(croppedImage)\n",
    "                if iClass == 'NonSpheroid':\n",
    "                    testLabels.append([0])\n",
    "                if iClass == 'Spheroid':\n",
    "                    testLabels.append([1])\n",
    "            if iSet == 'Training':\n",
    "                trainingSpheroids.append(croppedImage)\n",
    "                if iClass == 'NonSpheroid':\n",
    "                    trainingLabels.append([0])\n",
    "                if iClass == 'Spheroid':\n",
    "                    trainingLabels.append([1])\n",
    "            if iSet == 'Validation':\n",
    "                validationSpheroids.append(croppedImage)\n",
    "                if iClass == 'NonSpheroid':\n",
    "                    validationLabels.append([0])\n",
    "                if iClass == 'Spheroid':\n",
    "                    validationLabels.append([1])\n",
    "             \n",
    "maximumImageSize=0\n",
    "#maximumImageSize =  max(allSizes)\n",
    "#print(max(allSizes))          \n",
    "segmentationData = { \"testSpheroids\": testSpheroids, \"testLabels\": testLabels,\"trainingSpheroids\": trainingSpheroids, \"trainingLabels\": trainingLabels,\"validationSpheroids\": validationSpheroids, \"validationLabels\": validationLabels,\"maximumImageSize\": maximumImageSize}\n",
    "pickle.dump( segmentationData, open( \"segmentationData2Classes.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphHealthy\\20160126-corning-all-spheroids-p5-007hps_M01_w1.TIF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "segmentSpheroid.py:55: VisibleDeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future\n",
      "  selectedCC = (labeledImage == allProperties[indexOfMinDistance].label)\n",
      "segmentSpheroid.py:59: VisibleDeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future\n",
      "  boundingBox =  allProperties[indexOfMinDistance].bbox\n",
      "segmentSpheroid.py:62: VisibleDeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future\n",
      "  centroid = allProperties[indexOfMinDistance].centroid\n",
      "segmentSpheroid.py:63: VisibleDeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future\n",
      "  perimeter = allProperties[indexOfMinDistance].perimeter\n",
      "segmentSpheroid.py:64: VisibleDeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future\n",
      "  area = allProperties[indexOfMinDistance].area\n",
      "segmentSpheroid.py:65: VisibleDeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future\n",
      "  diameter = allProperties[indexOfMinDistance].equivalent_diameter\n",
      "segmentSpheroid.py:66: VisibleDeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future\n",
      "  majorAxis = allProperties[indexOfMinDistance].major_axis_length\n",
      "segmentSpheroid.py:67: VisibleDeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future\n",
      "  minorAxis = allProperties[indexOfMinDistance].minor_axis_length\n",
      "segmentSpheroid.py:79: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  croppedBWImage = spheroidBWImage[minRow:maxRow,minCol:maxCol]\n",
      "segmentSpheroid.py:80: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  croppedImage  =  inputImage[minRow:maxRow,minCol:maxCol]*croppedBWImage\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphHealthy\\20160127-corning-all-spheroids-p1-020hps_L01_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphHealthy\\20160127-corning-all-spheroids-p1-020hps_H06_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphHealthy\\20160127-corning-all-spheroids-p1-020hps_F05_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphHealthy\\20160127-corning-all-spheroids-p1-020hps_D03_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphHealthy\\20160127-corning-all-spheroids-p1-020hps_B05_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphHealthy\\20160126-corning-all-spheroids-p4-006hps_D05_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphHealthy\\20160126-corning-all-spheroids-p4-006hps_F05_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphHealthy\\20160126-corning-all-spheroids-p4-006hps_I03_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphHealthy\\20160126-corning-all-spheroids-p2-004hps_D03_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphHealthy\\20160126-corning-all-spheroids-p2-004hps_E06_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphHealthy\\20160126-corning-all-spheroids-p2-004hps_F04_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphHealthy\\20160126-corning-all-spheroids-p2-004hps_G03_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphHealthy\\20160126-corning-all-spheroids-p2-004hps_H01_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphHealthy\\20160126-corning-all-spheroids-p2-004hps_H06_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphHealthy\\20160127-corning-all-spheroids-p1-020hps_D05_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphHealthy\\20160127-corning-all-spheroids-p1-020hps_E03_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphHealthy\\20160127-corning-all-spheroids-p1-020hps_G03_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphHealthy\\20160127-corning-all-spheroids-p1-020hps_O04_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphHealthy\\20160127-corning-all-spheroids-p1-020hps_O02_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphNonHealthy\\20160205-corning-all-spheroids-p4-242hps_F05_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphNonHealthy\\20160205-corning-all-spheroids-p4-242hps_G02_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphNonHealthy\\20160205-corning-all-spheroids-p4-242hps_G04_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphNonHealthy\\20160205-corning-all-spheroids-p4-242hps_G05_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphNonHealthy\\20160205-corning-all-spheroids-p4-242hps_H02_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphNonHealthy\\20160205-corning-all-spheroids-p4-242hps_H03_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphNonHealthy\\20160205-corning-all-spheroids-p4-242hps_H05_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphNonHealthy\\20160205-corning-all-spheroids-p4-242hps_I02_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphNonHealthy\\20160205-corning-all-spheroids-p4-242hps_I03_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphNonHealthy\\20160205-corning-all-spheroids-p4-242hps_I04_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphNonHealthy\\20160204-corning-all-spheroids-p4-218hps_G05_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphNonHealthy\\20160204-corning-all-spheroids-p4-218hps_H02_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphNonHealthy\\20160204-corning-all-spheroids-p4-218hps_I02_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphNonHealthy\\20160204-corning-all-spheroids-p4-218hps_I03_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphNonHealthy\\20160204-corning-all-spheroids-p4-218hps_I04_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphNonHealthy\\20160204-corning-all-spheroids-p4-218hps_K04_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphNonHealthy\\20160204-corning-all-spheroids-p4-218hps_M04_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphNonHealthy\\20160204-corning-all-spheroids-p4-218hps_L02_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphNonHealthy\\20160204-corning-all-spheroids-p4-218hps_L03_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphNonHealthy\\20160204-corning-all-spheroids-p4-218hps_L04_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\SphHealthy\\20160129-corning-all-spheroids-p4-072hps_N05_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\SphHealthy\\20160130-corning-all-spheroids-p4-097hps_M03_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\SphHealthy\\20160205-corning-all-spheroids-p1-237hps_L06_w4.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\SphHealthy\\20160129-corning-all-spheroids-p1-069hps_O04_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\SphHealthy\\20160130-corning-all-spheroids-p2-095hps_O05_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\SphHealthy\\20160128-corning-all-spheroids-p3-050hps_K04_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\SphHealthy\\20160128-corning-all-spheroids-p3-050hps_M03_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\SphHealthy\\20160205-corning-all-spheroids-p4-242hps_G01_w5.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\SphHealthy\\20160205-corning-all-spheroids-p4-242hps_H01_w5.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\SphHealthy\\20160130-corning-all-spheroids-p5-098hps_E01_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\SphHealthy\\20160130-corning-all-spheroids-p5-098hps_E06_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\SphHealthy\\20160130-corning-all-spheroids-p5-098hps_F01_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\SphHealthy\\20160130-corning-all-spheroids-p5-098hps_F06_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\SphHealthy\\20160131-corning-all-spheroids-p5-124hps_C06_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\SphHealthy\\20160131-corning-all-spheroids-p5-124hps_D01_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\SphHealthy\\20160131-corning-all-spheroids-p5-124hps_D06_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\SphHealthy\\20160131-corning-all-spheroids-p5-124hps_E01_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\SphHealthy\\20160202-corning-all-spheroids-p5-171hps_N06_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\SphHealthy\\20160202-corning-all-spheroids-p5-171hps_O06_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\SphHealthy\\20160203-corning-all-spheroids-p5-196hps_G06_w4.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\SphNonHealthy\\20160201-corning-all-spheroids-p4-146hps_N05_w5.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\SphNonHealthy\\20160205-corning-all-spheroids-p2-238hps_O05_w5.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\SphNonHealthy\\20160130-corning-all-spheroids-p5-098hps_E02_w3.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\SphNonHealthy\\20160130-corning-all-spheroids-p5-098hps_E03_w3.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\SphNonHealthy\\20160130-corning-all-spheroids-p5-098hps_E05_w3.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\SphNonHealthy\\20160130-corning-all-spheroids-p5-098hps_F02_w3.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\SphNonHealthy\\20160130-corning-all-spheroids-p5-098hps_F03_w3.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\SphNonHealthy\\20160130-corning-all-spheroids-p5-098hps_F04_w3.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\SphNonHealthy\\20160130-corning-all-spheroids-p5-098hps_F05_w3.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\SphNonHealthy\\20160131-corning-all-spheroids-p5-124hps_C02_w3.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\SphNonHealthy\\20160131-corning-all-spheroids-p5-124hps_C03_w3.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\SphNonHealthy\\20160131-corning-all-spheroids-p5-124hps_C04_w3.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\SphNonHealthy\\20160131-corning-all-spheroids-p5-124hps_C05_w3.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\SphNonHealthy\\20160131-corning-all-spheroids-p5-124hps_D02_w3.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\SphNonHealthy\\20160131-corning-all-spheroids-p5-124hps_D03_w3.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\SphNonHealthy\\20160131-corning-all-spheroids-p5-124hps_D04_w3.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\SphNonHealthy\\20160131-corning-all-spheroids-p5-124hps_D05_w3.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\SphNonHealthy\\20160131-corning-all-spheroids-p5-124hps_E02_w3.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\SphNonHealthy\\20160202-corning-all-spheroids-p5-171hps_N02_w3.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Training\\NonSphHealthy\\20160126-corning-all-spheroids-p5-007hps_A03_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Training\\NonSphHealthy\\20160127-corning-all-spheroids-p1-020hps_P02_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Training\\NonSphHealthy\\20160127-corning-all-spheroids-p1-020hps_K01_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Training\\NonSphHealthy\\20160127-corning-all-spheroids-p1-020hps_H02_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Training\\NonSphHealthy\\20160127-corning-all-spheroids-p1-020hps_F01_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Training\\NonSphHealthy\\20160127-corning-all-spheroids-p1-020hps_D06_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Training\\NonSphHealthy\\20160126-corning-all-spheroids-p4-006hps_B06_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Training\\NonSphHealthy\\20160126-corning-all-spheroids-p4-006hps_E02_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Training\\NonSphHealthy\\20160126-corning-all-spheroids-p4-006hps_G05_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Training\\NonSphHealthy\\20160126-corning-all-spheroids-p2-004hps_C04_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Training\\NonSphNonHealthy\\20160205-corning-all-spheroids-p4-242hps_M03_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Training\\NonSphNonHealthy\\20160205-corning-all-spheroids-p4-242hps_A02_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Training\\NonSphNonHealthy\\20160205-corning-all-spheroids-p4-242hps_A03_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Training\\NonSphNonHealthy\\20160205-corning-all-spheroids-p4-242hps_A04_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Training\\NonSphNonHealthy\\20160205-corning-all-spheroids-p4-242hps_A05_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Training\\NonSphNonHealthy\\20160204-corning-all-spheroids-p4-218hps_A02_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Training\\NonSphNonHealthy\\20160204-corning-all-spheroids-p4-218hps_P05_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Training\\NonSphNonHealthy\\20160204-corning-all-spheroids-p4-218hps_O05_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Training\\NonSphNonHealthy\\20160204-corning-all-spheroids-p4-218hps_O02_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Training\\NonSphNonHealthy\\20160204-corning-all-spheroids-p4-218hps_N04_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Training\\SphHealthy\\20160129-corning-all-spheroids-p4-072hps_M03_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Training\\SphHealthy\\20160130-corning-all-spheroids-p1-094hps_L06_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Training\\SphHealthy\\20160129-corning-all-spheroids-p2-070hps_O05_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Training\\SphHealthy\\20160128-corning-all-spheroids-p3-050hps_H05_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Training\\SphHealthy\\20160128-corning-all-spheroids-p3-050hps_K06_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Training\\SphHealthy\\20160205-corning-all-spheroids-p4-242hps_A06_w5.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Training\\SphHealthy\\20160205-corning-all-spheroids-p4-242hps_D01_w5.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Training\\SphHealthy\\20160131-corning-all-spheroids-p4-122hps_B06_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Training\\SphHealthy\\20160202-corning-all-spheroids-p4-170hps_E06_w5.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Training\\SphHealthy\\20160202-corning-all-spheroids-p4-170hps_P06_w5.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Training\\SphNonHealthy\\20160201-corning-all-spheroids-p1-141hps_O04_w4.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Training\\SphNonHealthy\\20160202-corning-all-spheroids-p2-166hps_O05_w3.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Training\\SphNonHealthy\\20160201-corning-all-spheroids-p4-146hps_A02_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Training\\SphNonHealthy\\20160130-corning-all-spheroids-p5-098hps_A02_w3.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Training\\SphNonHealthy\\20160130-corning-all-spheroids-p5-098hps_A04_w3.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Training\\SphNonHealthy\\20160131-corning-all-spheroids-p5-124hps_A03_w3.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Training\\SphNonHealthy\\20160131-corning-all-spheroids-p5-124hps_A04_w3.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Training\\SphNonHealthy\\20160201-corning-all-spheroids-p5-149hps_A02_w3.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Training\\SphNonHealthy\\20160202-corning-all-spheroids-p5-171hps_B02_w3.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphHealthy\\20160126-corning-all-spheroids-p5-007hps_E05_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphHealthy\\20160127-corning-all-spheroids-p1-020hps_N04_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphHealthy\\20160127-corning-all-spheroids-p1-020hps_I06_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphHealthy\\20160127-corning-all-spheroids-p1-020hps_G04_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphHealthy\\20160127-corning-all-spheroids-p1-020hps_E05_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphHealthy\\20160127-corning-all-spheroids-p1-020hps_C02_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphHealthy\\20160126-corning-all-spheroids-p4-006hps_C05_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphHealthy\\20160126-corning-all-spheroids-p4-006hps_E06_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphHealthy\\20160126-corning-all-spheroids-p4-006hps_H04_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphHealthy\\20160126-corning-all-spheroids-p2-004hps_C06_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphHealthy\\20160126-corning-all-spheroids-p2-004hps_E03_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphHealthy\\20160126-corning-all-spheroids-p2-004hps_F01_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphHealthy\\20160126-corning-all-spheroids-p2-004hps_I03_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphHealthy\\20160126-corning-all-spheroids-p2-004hps_I04_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphHealthy\\20160126-corning-all-spheroids-p2-004hps_K02_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphHealthy\\20160127-corning-all-spheroids-p1-020hps_A04_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphHealthy\\20160127-corning-all-spheroids-p1-020hps_A06_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphHealthy\\20160127-corning-all-spheroids-p1-020hps_B03_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphHealthy\\20160127-corning-all-spheroids-p1-020hps_C03_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphHealthy\\20160127-corning-all-spheroids-p1-020hps_C05_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphNonHealthy\\20160205-corning-all-spheroids-p4-242hps_N05_w3.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphNonHealthy\\20160205-corning-all-spheroids-p4-242hps_B02_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphNonHealthy\\20160205-corning-all-spheroids-p4-242hps_B03_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphNonHealthy\\20160205-corning-all-spheroids-p4-242hps_B04_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphNonHealthy\\20160205-corning-all-spheroids-p4-242hps_C02_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphNonHealthy\\20160205-corning-all-spheroids-p4-242hps_C04_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphNonHealthy\\20160205-corning-all-spheroids-p4-242hps_C05_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphNonHealthy\\20160205-corning-all-spheroids-p4-242hps_E03_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphNonHealthy\\20160205-corning-all-spheroids-p4-242hps_F03_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphNonHealthy\\20160205-corning-all-spheroids-p4-242hps_F04_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphNonHealthy\\20160204-corning-all-spheroids-p4-218hps_B02_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphNonHealthy\\20160204-corning-all-spheroids-p4-218hps_B03_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphNonHealthy\\20160204-corning-all-spheroids-p4-218hps_B05_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphNonHealthy\\20160204-corning-all-spheroids-p4-218hps_C05_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphNonHealthy\\20160204-corning-all-spheroids-p4-218hps_D05_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphNonHealthy\\20160204-corning-all-spheroids-p4-218hps_E03_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphNonHealthy\\20160204-corning-all-spheroids-p4-218hps_F03_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphNonHealthy\\20160204-corning-all-spheroids-p4-218hps_F04_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphNonHealthy\\20160204-corning-all-spheroids-p4-218hps_G02_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphNonHealthy\\20160204-corning-all-spheroids-p4-218hps_G04_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphHealthy\\20160130-corning-all-spheroids-p4-097hps_N05_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphHealthy\\20160129-corning-all-spheroids-p1-069hps_L06_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphHealthy\\20160201-corning-all-spheroids-p1-141hps_L06_w4.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphHealthy\\20160130-corning-all-spheroids-p1-094hps_O04_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphHealthy\\20160128-corning-all-spheroids-p3-050hps_J05_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphHealthy\\20160128-corning-all-spheroids-p3-050hps_L03_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphHealthy\\20160205-corning-all-spheroids-p4-242hps_B06_w5.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphHealthy\\20160205-corning-all-spheroids-p4-242hps_C06_w5.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphHealthy\\20160130-corning-all-spheroids-p5-098hps_C06_w3.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphHealthy\\20160130-corning-all-spheroids-p5-098hps_D01_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphHealthy\\20160130-corning-all-spheroids-p5-098hps_D06_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphHealthy\\20160203-corning-all-spheroids-p5-196hps_H06_w4.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphHealthy\\20160203-corning-all-spheroids-p5-196hps_H01_w4.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphHealthy\\20160202-corning-all-spheroids-p5-171hps_M01_w4.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphHealthy\\20160202-corning-all-spheroids-p5-171hps_M06_w4.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphHealthy\\20160202-corning-all-spheroids-p5-171hps_L01_w4.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphHealthy\\20160201-corning-all-spheroids-p5-149hps_D01_w4.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphHealthy\\20160201-corning-all-spheroids-p5-149hps_D06_w4.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphHealthy\\20160201-corning-all-spheroids-p5-149hps_E01_w4.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphHealthy\\20160131-corning-all-spheroids-p5-124hps_A01_w1.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphNonHealthy\\20160201-corning-all-spheroids-p4-146hps_M03_w5.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphNonHealthy\\20160205-corning-all-spheroids-p1-237hps_O04_w4.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphNonHealthy\\20160130-corning-all-spheroids-p5-098hps_C02_w3.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphNonHealthy\\20160130-corning-all-spheroids-p5-098hps_C03_w3.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphNonHealthy\\20160130-corning-all-spheroids-p5-098hps_C04_w3.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphNonHealthy\\20160130-corning-all-spheroids-p5-098hps_C05_w3.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphNonHealthy\\20160130-corning-all-spheroids-p5-098hps_D02_w3.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphNonHealthy\\20160130-corning-all-spheroids-p5-098hps_D03_w3.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphNonHealthy\\20160130-corning-all-spheroids-p5-098hps_D04_w3.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphNonHealthy\\20160130-corning-all-spheroids-p5-098hps_D05_w3.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphNonHealthy\\20160203-corning-all-spheroids-p5-196hps_H05_w3.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphNonHealthy\\20160203-corning-all-spheroids-p5-196hps_H04_w3.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphNonHealthy\\20160203-corning-all-spheroids-p5-196hps_H02_w4.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphNonHealthy\\20160202-corning-all-spheroids-p5-171hps_N03_w3.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphNonHealthy\\20160202-corning-all-spheroids-p5-171hps_N05_w3.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphNonHealthy\\20160202-corning-all-spheroids-p5-171hps_M03_w3.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphNonHealthy\\20160202-corning-all-spheroids-p5-171hps_M04_w3.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphNonHealthy\\20160202-corning-all-spheroids-p5-171hps_L02_w3.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphNonHealthy\\20160201-corning-all-spheroids-p5-149hps_D02_w3.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphNonHealthy\\20160201-corning-all-spheroids-p5-149hps_D03_w3.TIF\n",
      "N:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphNonHealthy\\20160201-corning-all-spheroids-p5-149hps_D04_w3.TIF\n",
      "1358\n"
     ]
    }
   ],
   "source": [
    "##############################################\n",
    "#################4 CLASSES####################\n",
    "##############################################\n",
    "\"\"\"\n",
    "Demo for Morphosphere\n",
    "\n",
    "@author: Vardan Andriasyan\n",
    "\"\"\"\n",
    "\n",
    "#\n",
    "parentDir = 'N:\\\\Vardan_Andriasyan\\\\Morphosphere\\\\HT-29\\\\'\n",
    "sets = ['Test','Training','Validation']#'Test','Training','Validation'\n",
    "classes = ['NonSphHealthy','NonSphNonHealthy','SphHealthy','SphNonHealthy']#'NonSphHealthy','NonSphNonHealthy','SphHealthy','SphNonHealthy'\n",
    "\n",
    "#inputImage= 'Z:\\\\Vardan_Andriasyan\\\\Morphosphere\\\\HT-29\\\\Test\\\\NonSphNonHealthy\\\\20160205-corning-all-spheroids-p4-242hps_G05_w1.TIF'\n",
    "#fullImage, croppedImage, croppedBWImage, spheroidAttributes = segmentSpheroid(inputImage, 12, 501, 20000)\n",
    "\n",
    "trainingSpheroids = []\n",
    "testSpheroids = []\n",
    "validationSpheroids = []\n",
    "trainingLabels = []\n",
    "testLabels = []\n",
    "validationLabels = []\n",
    "\n",
    "allSizes = []\n",
    "\n",
    "for iSet in sets:\n",
    "    for iClass in classes:\n",
    "        inputFolderPath = parentDir+'\\\\'+iSet+'\\\\'+iClass\n",
    "        for currentImagePath in glob.glob(inputFolderPath+'\\\\*.TIF'):\n",
    "            print(currentImagePath)\n",
    "            croppedImage, croppedBWImage, spheroidAttributes = segmentSpheroid(currentImagePath, 20, 501, 20000)\n",
    "            #populate an array with all image sizes\n",
    "            imageHeight, imageWidth = croppedImage.shape[:2]\n",
    "            allSizes.append(max([imageHeight,imageWidth]))\n",
    "            if iSet == 'Test':\n",
    "                testSpheroids.append(croppedImage)\n",
    "                if iClass == 'NonSphHealthy':\n",
    "                    testLabels.append([0])\n",
    "                if iClass == 'NonSphNonHealthy':\n",
    "                    testLabels.append([1])\n",
    "                if iClass == 'SphHealthy':\n",
    "                    testLabels.append([2])\n",
    "                if iClass == 'SphNonHealthy':\n",
    "                    testLabels.append([3])\n",
    "            if iSet == 'Training':\n",
    "                trainingSpheroids.append(croppedImage)\n",
    "                if iClass == 'NonSphHealthy':\n",
    "                    trainingLabels.append([0])\n",
    "                if iClass == 'NonSphNonHealthy':\n",
    "                    trainingLabels.append([1])\n",
    "                if iClass == 'SphHealthy':\n",
    "                    trainingLabels.append([2])\n",
    "                if iClass == 'SphNonHealthy':\n",
    "                    trainingLabels.append([3])\n",
    "            if iSet == 'Validation':\n",
    "                validationSpheroids.append(croppedImage)\n",
    "                if iClass == 'NonSphHealthy':\n",
    "                    validationLabels.append([0])\n",
    "                if iClass == 'NonSphNonHealthy':\n",
    "                    validationLabels.append([1])\n",
    "                if iClass == 'SphHealthy':\n",
    "                    validationLabels.append([2])\n",
    "                if iClass == 'SphNonHealthy':\n",
    "                    validationLabels.append([3])\n",
    "\n",
    "maximumImageSize =  max(allSizes)\n",
    "print(max(allSizes))          \n",
    "segmentationData = { \"testSpheroids\": testSpheroids, \"testLabels\": testLabels,\"trainingSpheroids\": trainingSpheroids, \"trainingLabels\": trainingLabels,\"validationSpheroids\": validationSpheroids, \"validationLabels\": validationLabels,\"maximumImageSize\": maximumImageSize}\n",
    "pickle.dump( segmentationData, open( \"segmentationData4Classes.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-a8dec0980051>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mnumberOfReplicates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mtrainingSet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerateBatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msegmentationData\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"trainingSpheroids\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msegmentationData\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"trainingLabels\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaximumImageSize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputImageSize\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnumberOfReplicates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mtestingSet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerateBatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msegmentationData\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"testSpheroids\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msegmentationData\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"testLabels\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaximumImageSize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputImageSize\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnumberOfReplicates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mvalidationSet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerateBatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msegmentationData\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"validationSpheroids\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msegmentationData\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"validationLabels\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaximumImageSize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputImageSize\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnumberOfReplicates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\FannyG\\Documents\\GitHub\\Morphosphere.py\\generateBatch.pyc\u001b[0m in \u001b[0;36mgenerateBatch\u001b[1;34m(currentSpheroidSet, currentLabelSet, maximumImageSize, outputImageSize, numberOfReplicates)\u001b[0m\n\u001b[0;32m     72\u001b[0m                 \u001b[0mcurrentTransformedImage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrentRescaledImage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputImageSize\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0moutputImageSize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m                 \u001b[0mcurrentBatchSpheroids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrentBatchSpheroids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcurrentTransformedImage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m                 \u001b[0mcurrentBatchLabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrentBatchLabels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcurrentLabelSet\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0miLabel\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m             \u001b[0mi\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[0miLabel\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\FannyG\\Documents\\GitHub\\Morphosphere.py\\generateBatch.pyc\u001b[0m in \u001b[0;36mrandomtransform\u001b[1;34m(inputImage)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[0moutputImage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandomtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msegmentationData\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'testSpheroids'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msegmentationData\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'testSpheroids'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputImage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\FannyG\\Documents\\Anaconda-2\\lib\\site-packages\\skimage\\transform\\_warps.pyc\u001b[0m in \u001b[0;36mrotate\u001b[1;34m(image, angle, resize, center, order, mode, cval, clip, preserve_range)\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m     return warp(image, tform, output_shape=output_shape, order=order,\n\u001b[1;32m--> 283\u001b[1;33m                 mode=mode, cval=cval, clip=clip, preserve_range=preserve_range)\n\u001b[0m\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\FannyG\\Documents\\Anaconda-2\\lib\\site-packages\\skimage\\transform\\_geometric.pyc\u001b[0m in \u001b[0;36mwarp\u001b[1;34m(image, inverse_map, map_args, output_shape, order, mode, cval, clip, preserve_range)\u001b[0m\n\u001b[0;32m   1341\u001b[0m                 warped = _warp_fast(image, matrix,\n\u001b[0;32m   1342\u001b[0m                                  \u001b[0moutput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1343\u001b[1;33m                                  order=order, mode=mode, cval=cval)\n\u001b[0m\u001b[0;32m   1344\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1345\u001b[0m                 \u001b[0mdims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mskimage/transform/_warps_cy.pyx\u001b[0m in \u001b[0;36mskimage.transform._warps_cy._warp_fast (skimage\\transform\\_warps_cy.c:2637)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\FannyG\\Documents\\Anaconda-2\\lib\\site-packages\\numpy\\core\\numeric.pyc\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    412\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    413\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 414\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    415\u001b[0m     \"\"\"Convert the input to an array.\n\u001b[0;32m    416\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#load segmentation DATA\n",
    "from generateBatch import generateBatch\n",
    "#segmentationData = pickle.load( open( \"segmentationData2Classes.p\", \"rb\" ) )\n",
    "segmentationData = pickle.load( open( \"segmentationData4Classes.p\", \"rb\" ) )\n",
    "maximumImageSize = segmentationData[\"maximumImageSize\"]\n",
    "#numberOfClasses = 2\n",
    "numberOfClasses = 4\n",
    "outputImageSize = 28\n",
    "\n",
    "# \n",
    "numberOfReplicates = 10\n",
    "trainingSet = generateBatch(segmentationData[\"trainingSpheroids\"],segmentationData[\"trainingLabels\"], maximumImageSize, outputImageSize,numberOfReplicates)\n",
    "testingSet = generateBatch(segmentationData[\"testSpheroids\"],segmentationData[\"testLabels\"], maximumImageSize, outputImageSize,numberOfReplicates)\n",
    "validationSet = generateBatch(segmentationData[\"validationSpheroids\"],segmentationData[\"validationLabels\"], maximumImageSize, outputImageSize,numberOfReplicates)\n",
    "\n",
    "cnnData = {\"testingSet\": testingSet,\"trainingSet\": trainingSet,\"validationSet\": validationSet}\n",
    "pickle.dump( cnnData, open( \"cnnData.p\", \"wb\" ) )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "cnnData = pickle.load( open( \"cnnData.p\", \"rb\" ) )\n",
    "validationSet = cnnData[\"trainingSet\"]\n",
    "testingSet = cnnData[\"testingSet\"] \n",
    "trainingSet =  cnnData[\"validationSet\"]\n",
    "outputImageSize = 28\n",
    "numberOfClasses = 2\n",
    "print(len(trainingSet[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(<TensorType(float64, matrix)>, Elemwise{Cast{int32}}.0), (<TensorType(float64, matrix)>, Elemwise{Cast{int32}}.0), (<TensorType(float64, matrix)>, Elemwise{Cast{int32}}.0)]\n",
      "... building the model\n",
      "... training\n",
      "('training @ iter = ', 0L)\n",
      "epoch 1, minibatch 1/1, validation error 50.000000 %\n",
      "     epoch 1, minibatch 1/1, test error of best model 50.000000 %\n",
      "epoch 2, minibatch 1/1, validation error 50.000000 %\n",
      "epoch 3, minibatch 1/1, validation error 50.000000 %\n",
      "epoch 4, minibatch 1/1, validation error 50.000000 %\n",
      "epoch 5, minibatch 1/1, validation error 50.000000 %\n",
      "epoch 6, minibatch 1/1, validation error 50.000000 %\n",
      "epoch 7, minibatch 1/1, validation error 50.000000 %\n",
      "epoch 8, minibatch 1/1, validation error 50.000000 %\n",
      "epoch 9, minibatch 1/1, validation error 50.000000 %\n",
      "epoch 10, minibatch 1/1, validation error 50.000000 %\n",
      "epoch 11, minibatch 1/1, validation error 50.000000 %\n",
      "epoch 12, minibatch 1/1, validation error 50.000000 %\n",
      "epoch 13, minibatch 1/1, validation error 50.000000 %\n",
      "epoch 14, minibatch 1/1, validation error 50.000000 %\n",
      "epoch 15, minibatch 1/1, validation error 50.000000 %\n",
      "epoch 16, minibatch 1/1, validation error 50.000000 %\n",
      "epoch 17, minibatch 1/1, validation error 50.000000 %\n",
      "epoch 18, minibatch 1/1, validation error 50.000000 %\n",
      "epoch 19, minibatch 1/1, validation error 50.000000 %\n",
      "epoch 20, minibatch 1/1, validation error 46.000000 %\n",
      "     epoch 20, minibatch 1/1, test error of best model 50.000000 %\n",
      "epoch 21, minibatch 1/1, validation error 45.000000 %\n",
      "     epoch 21, minibatch 1/1, test error of best model 50.000000 %\n",
      "epoch 22, minibatch 1/1, validation error 40.000000 %\n",
      "     epoch 22, minibatch 1/1, test error of best model 50.000000 %\n",
      "epoch 23, minibatch 1/1, validation error 37.000000 %\n",
      "     epoch 23, minibatch 1/1, test error of best model 50.000000 %\n",
      "epoch 24, minibatch 1/1, validation error 37.000000 %\n",
      "epoch 25, minibatch 1/1, validation error 35.000000 %\n",
      "     epoch 25, minibatch 1/1, test error of best model 47.000000 %\n",
      "epoch 26, minibatch 1/1, validation error 34.000000 %\n",
      "     epoch 26, minibatch 1/1, test error of best model 44.000000 %\n",
      "epoch 27, minibatch 1/1, validation error 31.000000 %\n",
      "     epoch 27, minibatch 1/1, test error of best model 41.000000 %\n",
      "epoch 28, minibatch 1/1, validation error 30.000000 %\n",
      "     epoch 28, minibatch 1/1, test error of best model 39.000000 %\n",
      "epoch 29, minibatch 1/1, validation error 30.000000 %\n",
      "epoch 30, minibatch 1/1, validation error 30.000000 %\n",
      "epoch 31, minibatch 1/1, validation error 30.000000 %\n",
      "epoch 32, minibatch 1/1, validation error 30.000000 %\n",
      "epoch 33, minibatch 1/1, validation error 30.000000 %\n",
      "epoch 34, minibatch 1/1, validation error 30.000000 %\n",
      "epoch 35, minibatch 1/1, validation error 30.000000 %\n",
      "epoch 36, minibatch 1/1, validation error 30.000000 %\n",
      "epoch 37, minibatch 1/1, validation error 30.000000 %\n",
      "epoch 38, minibatch 1/1, validation error 28.000000 %\n",
      "     epoch 38, minibatch 1/1, test error of best model 25.000000 %\n",
      "epoch 39, minibatch 1/1, validation error 28.000000 %\n",
      "epoch 40, minibatch 1/1, validation error 25.000000 %\n",
      "     epoch 40, minibatch 1/1, test error of best model 19.000000 %\n",
      "epoch 41, minibatch 1/1, validation error 24.000000 %\n",
      "     epoch 41, minibatch 1/1, test error of best model 17.000000 %\n",
      "epoch 42, minibatch 1/1, validation error 19.000000 %\n",
      "     epoch 42, minibatch 1/1, test error of best model 12.000000 %\n",
      "epoch 43, minibatch 1/1, validation error 17.000000 %\n",
      "     epoch 43, minibatch 1/1, test error of best model 7.000000 %\n",
      "epoch 44, minibatch 1/1, validation error 17.000000 %\n",
      "epoch 45, minibatch 1/1, validation error 15.000000 %\n",
      "     epoch 45, minibatch 1/1, test error of best model 3.000000 %\n",
      "epoch 46, minibatch 1/1, validation error 13.000000 %\n",
      "     epoch 46, minibatch 1/1, test error of best model 2.000000 %\n",
      "epoch 47, minibatch 1/1, validation error 11.000000 %\n",
      "     epoch 47, minibatch 1/1, test error of best model 1.000000 %\n",
      "epoch 48, minibatch 1/1, validation error 10.000000 %\n",
      "     epoch 48, minibatch 1/1, test error of best model 1.000000 %\n",
      "epoch 49, minibatch 1/1, validation error 10.000000 %\n",
      "epoch 50, minibatch 1/1, validation error 10.000000 %\n",
      "epoch 51, minibatch 1/1, validation error 10.000000 %\n",
      "epoch 52, minibatch 1/1, validation error 9.000000 %\n",
      "     epoch 52, minibatch 1/1, test error of best model 0.000000 %\n",
      "epoch 53, minibatch 1/1, validation error 8.000000 %\n",
      "     epoch 53, minibatch 1/1, test error of best model 0.000000 %\n",
      "epoch 54, minibatch 1/1, validation error 7.000000 %\n",
      "     epoch 54, minibatch 1/1, test error of best model 0.000000 %\n",
      "epoch 55, minibatch 1/1, validation error 6.000000 %\n",
      "     epoch 55, minibatch 1/1, test error of best model 0.000000 %\n",
      "epoch 56, minibatch 1/1, validation error 6.000000 %\n",
      "epoch 57, minibatch 1/1, validation error 5.000000 %\n",
      "     epoch 57, minibatch 1/1, test error of best model 0.000000 %\n",
      "epoch 58, minibatch 1/1, validation error 3.000000 %\n",
      "     epoch 58, minibatch 1/1, test error of best model 0.000000 %\n",
      "epoch 59, minibatch 1/1, validation error 2.000000 %\n",
      "     epoch 59, minibatch 1/1, test error of best model 0.000000 %\n",
      "epoch 60, minibatch 1/1, validation error 1.000000 %\n",
      "     epoch 60, minibatch 1/1, test error of best model 0.000000 %\n",
      "epoch 61, minibatch 1/1, validation error 1.000000 %\n",
      "epoch 62, minibatch 1/1, validation error 1.000000 %\n",
      "epoch 63, minibatch 1/1, validation error 1.000000 %\n",
      "epoch 64, minibatch 1/1, validation error 1.000000 %\n",
      "epoch 65, minibatch 1/1, validation error 0.000000 %\n",
      "     epoch 65, minibatch 1/1, test error of best model 0.000000 %\n",
      "epoch 66, minibatch 1/1, validation error 0.000000 %\n",
      "epoch 67, minibatch 1/1, validation error 0.000000 %\n",
      "epoch 68, minibatch 1/1, validation error 0.000000 %\n",
      "epoch 69, minibatch 1/1, validation error 0.000000 %\n",
      "epoch 70, minibatch 1/1, validation error 0.000000 %\n",
      "epoch 71, minibatch 1/1, validation error 0.000000 %\n",
      "epoch 72, minibatch 1/1, validation error 0.000000 %\n",
      "epoch 73, minibatch 1/1, validation error 0.000000 %\n",
      "epoch 74, minibatch 1/1, validation error 0.000000 %\n",
      "epoch 75, minibatch 1/1, validation error 0.000000 %\n",
      "epoch 76, minibatch 1/1, validation error 0.000000 %\n",
      "epoch 77, minibatch 1/1, validation error 0.000000 %\n",
      "epoch 78, minibatch 1/1, validation error 0.000000 %\n",
      "epoch 79, minibatch 1/1, validation error 0.000000 %\n",
      "epoch 80, minibatch 1/1, validation error 0.000000 %\n",
      "epoch 81, minibatch 1/1, validation error 0.000000 %\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-e576a631df7e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m \u001b[0mevaluate_lenet5\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnkerns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumberOfClasses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchannel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-e576a631df7e>\u001b[0m in \u001b[0;36mevaluate_lenet5\u001b[1;34m(learning_rate, n_epochs, nkerns, batch_size, numberOfClasses)\u001b[0m\n\u001b[0;32m    174\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0miter\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m                 \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'training @ iter = '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 176\u001b[1;33m             \u001b[0mcost_ij\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mminibatch_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0miter\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mvalidation_frequency\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\FannyG\\Documents\\Anaconda-2\\lib\\site-packages\\theano\\compile\\function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\FannyG\\Documents\\Anaconda-2\\lib\\site-packages\\theano\\gof\\vm.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    295\u001b[0m                         \u001b[0mold_s\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m             \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 297\u001b[1;33m                 \u001b[0mlink\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_with_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthunk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    298\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\FannyG\\Documents\\Anaconda-2\\lib\\site-packages\\theano\\gof\\link.pyc\u001b[0m in \u001b[0;36mraise_with_op\u001b[1;34m(node, thunk, exc_info, storage_map)\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mexc_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[1;31m# print a simple traceback from KeyboardInterrupt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m         \u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_trace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[0mtrace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\FannyG\\Documents\\Anaconda-2\\lib\\site-packages\\theano\\gof\\vm.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    291\u001b[0m                 for thunk, node, old_storage in zip(self.thunks, self.nodes,\n\u001b[0;32m    292\u001b[0m                                                     self.post_thunk_clear):\n\u001b[1;32m--> 293\u001b[1;33m                     \u001b[0mthunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    294\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0mold_s\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mold_storage\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m                         \u001b[0mold_s\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\FannyG\\Documents\\Anaconda-2\\lib\\site-packages\\theano\\gof\\op.pyc\u001b[0m in \u001b[0;36mrval\u001b[1;34m(p, i, o, n)\u001b[0m\n\u001b[0;32m    910\u001b[0m             \u001b[1;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    911\u001b[0m             \u001b[1;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 912\u001b[1;33m                 \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    913\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m                     \u001b[0mcompute_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\FannyG\\Documents\\Anaconda-2\\lib\\site-packages\\theano\\tensor\\nnet\\conv.pyc\u001b[0m in \u001b[0;36mperform\u001b[1;34m(self, node, inp, out)\u001b[0m\n\u001b[0;32m    804\u001b[0m                         \u001b[1;31m# some cast generates a warning here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    805\u001b[0m                         zz[b, n, ...] += _convolve2d(img2d[b, im0, ...],\n\u001b[1;32m--> 806\u001b[1;33m                                                      \u001b[0mfiltersflipped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mim0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    807\u001b[0m                                                      1, val, bval, 0)\n\u001b[0;32m    808\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from loadCNNData import loadCNNData\n",
    "from CNNClasses import HiddenLayer,LogisticRegression,LeNetConvPoolLayer\n",
    "import os\n",
    "import sys\n",
    "import timeit\n",
    "\n",
    "import numpy\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.tensor.signal import pool\n",
    "from theano.tensor.nnet import conv2d\n",
    "\n",
    "\n",
    "def evaluate_lenet5(learning_rate=0.5, n_epochs=200,\n",
    "                    nkerns=[20, 50], batch_size=100, numberOfClasses=4):\n",
    "   \n",
    "\n",
    "    rng = numpy.random.RandomState(23455)\n",
    "\n",
    "    datasets = loadCNNData(trainingSet,testingSet,validationSet)\n",
    "\n",
    "    train_set_x, train_set_y = datasets[0]\n",
    "    valid_set_x, valid_set_y = datasets[1]\n",
    "    test_set_x, test_set_y = datasets[2]\n",
    "\n",
    "    # compute number of minibatches for training, validation and testing\n",
    "    n_train_batches = train_set_x.get_value(borrow=True).shape[0]\n",
    "    n_valid_batches = valid_set_x.get_value(borrow=True).shape[0]\n",
    "    n_test_batches = test_set_x.get_value(borrow=True).shape[0]\n",
    "    n_train_batches //= batch_size\n",
    "    n_valid_batches //= batch_size\n",
    "    n_test_batches //= batch_size\n",
    "\n",
    "    # allocate symbolic variables for the data\n",
    "    index = T.lscalar()  # index to a [mini]batch\n",
    "\n",
    "    # start-snippet-1\n",
    "    x = T.matrix('x')   # the data is presented as rasterized images\n",
    "    y = T.ivector('y')  # the labels are presented as 1D vector of\n",
    "                        # [int] labels\n",
    "\n",
    "    ######################\n",
    "    # BUILD ACTUAL MODEL #\n",
    "    ######################\n",
    "    print('... building the model')\n",
    "\n",
    "    # Reshape matrix of rasterized images of shape (batch_size, 128 * 128)\n",
    "    # to a 4D tensor, compatible with our LeNetConvPoolLayer\n",
    "    # (128, 128) is the size of images.\n",
    "    layer0_input = x.reshape((batch_size, 1, 28, 28))\n",
    "\n",
    "    # Construct the first convolutional pooling layer:\n",
    "    # filtering reduces the image size to (28-5+1 , 28-5+1) = (24, 24)\n",
    "    # maxpooling reduces this further to (24/2, 24/2) = (12, 12)\n",
    "    # 4D output tensor is thus of shape (batch_size, nkerns[0], 12, 12)\n",
    "    layer0 = LeNetConvPoolLayer(\n",
    "        rng,\n",
    "        input=layer0_input,\n",
    "        image_shape=(batch_size, 1, 28, 28),\n",
    "        filter_shape=(nkerns[0], 1, 5, 5),\n",
    "        poolsize=(2, 2)\n",
    "    )\n",
    "\n",
    "    # Construct the second convolutional pooling layer\n",
    "    # filtering reduces the image size to (12-5+1, 12-5+1) = (8, 8)\n",
    "    # maxpooling reduces this further to (8/2, 8/2) = (4, 4)\n",
    "    # 4D output tensor is thus of shape (batch_size, nkerns[1], 4, 4)\n",
    "    layer1 = LeNetConvPoolLayer(\n",
    "        rng,\n",
    "        input=layer0.output,\n",
    "        image_shape=(batch_size, nkerns[0], 12, 12),\n",
    "        filter_shape=(nkerns[1], nkerns[0], 5, 5),\n",
    "        poolsize=(2, 2)\n",
    "    )\n",
    "\n",
    "    # the HiddenLayer being fully-connected, it operates on 2D matrices of\n",
    "    # shape (batch_size, num_pixels) (i.e matrix of rasterized images).\n",
    "    # This will generate a matrix of shape (batch_size, nkerns[1] * 4 * 4),\n",
    "    # or (500, 50 * 4 * 4) = (500, 800) with the default values.\n",
    "    layer2_input = layer1.output.flatten(2)\n",
    "\n",
    "    # construct a fully-connected sigmoidal layer\n",
    "    layer2 = HiddenLayer(\n",
    "        rng,\n",
    "        input=layer2_input,\n",
    "        n_in=nkerns[1] * 4 * 4,\n",
    "        n_out=batch_size,\n",
    "        activation=T.tanh\n",
    "    )\n",
    "\n",
    "    # classify the values of the fully-connected sigmoidal layer\n",
    "    layer3 = LogisticRegression(input=layer2.output, n_in=batch_size, n_out=numberOfClasses)\n",
    "\n",
    "    # the cost we minimize during training is the NLL of the model\n",
    "    cost = layer3.negative_log_likelihood(y)\n",
    "\n",
    "    # create a function to compute the mistakes that are made by the model\n",
    "    test_model = theano.function(\n",
    "        [index],\n",
    "        layer3.errors(y),\n",
    "        givens={\n",
    "            x: test_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "            y: test_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    validate_model = theano.function(\n",
    "        [index],\n",
    "        layer3.errors(y),\n",
    "        givens={\n",
    "            x: valid_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "            y: valid_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # create a list of all model parameters to be fit by gradient descent\n",
    "    params = layer3.params + layer2.params + layer1.params + layer0.params\n",
    "\n",
    "    # create a list of gradients for all model parameters\n",
    "    grads = T.grad(cost, params)\n",
    "\n",
    "    # train_model is a function that updates the model parameters by\n",
    "    # SGD Since this model has many parameters, it would be tedious to\n",
    "    # manually create an update rule for each model parameter. We thus\n",
    "    # create the updates list by automatically looping over all\n",
    "    # (params[i], grads[i]) pairs.\n",
    "    updates = [\n",
    "        (param_i, param_i - learning_rate * grad_i)\n",
    "        for param_i, grad_i in zip(params, grads)\n",
    "    ]\n",
    "\n",
    "    train_model = theano.function(\n",
    "        [index],\n",
    "        cost,\n",
    "        updates=updates,\n",
    "        givens={\n",
    "            x: train_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "            y: train_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "    # end-snippet-1\n",
    "\n",
    "    ###############\n",
    "    # TRAIN MODEL #\n",
    "    ###############\n",
    "    print('... training')\n",
    "    # early-stopping parameters\n",
    "    patience = 10000  # look as this many examples regardless\n",
    "    patience_increase = 1  # wait this much longer when a new best is\n",
    "                           # found\n",
    "    improvement_threshold = 0.995  # a relative improvement of this much is\n",
    "                                   # considered significant\n",
    "    validation_frequency = min(n_train_batches, patience // 2)\n",
    "                                  # go through this many\n",
    "                                  # minibatche before checking the network\n",
    "                                  # on the validation set; in this case we\n",
    "                                  # check every epoch\n",
    "\n",
    "    best_validation_loss = numpy.inf\n",
    "    best_iter = 0\n",
    "    test_score = 0.\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    epoch = 0\n",
    "    done_looping = False\n",
    "\n",
    "    while (epoch < n_epochs) and (not done_looping):\n",
    "        epoch = epoch + 1\n",
    "        for minibatch_index in range(n_train_batches):\n",
    "\n",
    "            iter = (epoch - 1) * n_train_batches + minibatch_index\n",
    "\n",
    "            if iter % 100 == 0:\n",
    "                print('training @ iter = ', iter)\n",
    "            cost_ij = train_model(minibatch_index)\n",
    "\n",
    "            if (iter + 1) % validation_frequency == 0:\n",
    "\n",
    "                # compute zero-one loss on validation set\n",
    "                validation_losses = [validate_model(i) for i\n",
    "                                     in range(n_valid_batches)]\n",
    "                this_validation_loss = numpy.mean(validation_losses)\n",
    "                print('epoch %i, minibatch %i/%i, validation error %f %%' %\n",
    "                      (epoch, minibatch_index + 1, n_train_batches,\n",
    "                       this_validation_loss * 100.))\n",
    "\n",
    "                # if we got the best validation score until now\n",
    "                if this_validation_loss < best_validation_loss:\n",
    "\n",
    "                    #improve patience if loss improvement is good enough\n",
    "                    if this_validation_loss < best_validation_loss *  \\\n",
    "                       improvement_threshold:\n",
    "                        patience = max(patience, iter * patience_increase)\n",
    "\n",
    "                    # save best validation score and iteration number\n",
    "                    best_validation_loss = this_validation_loss\n",
    "                    best_iter = iter\n",
    "\n",
    "                    # test it on the test set\n",
    "                    test_losses = [\n",
    "                        test_model(i)\n",
    "                        for i in range(n_test_batches)\n",
    "                    ]\n",
    "                    test_score = numpy.mean(test_losses)\n",
    "                    print(('     epoch %i, minibatch %i/%i, test error of '\n",
    "                           'best model %f %%') %\n",
    "                          (epoch, minibatch_index + 1, n_train_batches,\n",
    "                           test_score * 100.))\n",
    "\n",
    "            if patience <= iter:\n",
    "                done_looping = True\n",
    "                break\n",
    "\n",
    "    end_time = timeit.default_timer()\n",
    "    print('Optimization complete.')\n",
    "    print('Best validation score of %f %% obtained at iteration %i, '\n",
    "          'with test performance %f %%' %\n",
    "          (best_validation_loss * 100., best_iter + 1, test_score * 100.))\n",
    "    print(('The code for file ' + ' ran for %.2fm' % ((end_time - start_time) / 60.)))\n",
    "\n",
    "\n",
    "evaluate_lenet5(learning_rate=0.1, n_epochs=200,nkerns=[20, 50], batch_size=100, numberOfClasses=2)\n",
    "\n",
    "def experiment(state, channel):\n",
    "    evaluate_lenet5(state.learning_rate, dataset=state.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from logisticRegression import LogisticRegression\n",
    "from loadCNNData import loadCNNData\n",
    "\n",
    "\n",
    "\n",
    "def sgd_optimization_mnist(learning_rate=0.2, n_epochs=2000,dataset='mnist.pkl.gz',batch_size=10,imageSize=32,numberOfClasses=4):\n",
    "    \"\"\"\n",
    "    Demonstrate stochastic gradient descent optimization of a log-linear\n",
    "    model\n",
    "\n",
    "    :type learning_rate: float\n",
    "    :param learning_rate: learning rate used (factor for the stochastic\n",
    "                          gradient)\n",
    "\n",
    "    :type n_epochs: int\n",
    "    :param n_epochs: maximal number of epochs to run the optimizer\n",
    "\n",
    "    :type dataset: string\n",
    "    :param dataset: the path of the MNIST dataset file from\n",
    "                 http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz\n",
    "\n",
    "    \"\"\"\n",
    "    datasets = loadCNNData(trainingSet,testingSet,validationSet)\n",
    "    #print(datasets[2])\n",
    "    \n",
    "    train_set_x, train_set_y = datasets[0]\n",
    "    valid_set_x, valid_set_y = datasets[1]\n",
    "    test_set_x, test_set_y = datasets[2]\n",
    "\n",
    "    # compute number of minibatches for training, validation and testing\n",
    "    n_train_batches = train_set_x.get_value(borrow=True).shape[0] // batch_size\n",
    "    n_valid_batches = valid_set_x.get_value(borrow=True).shape[0] // batch_size\n",
    "    n_test_batches = test_set_x.get_value(borrow=True).shape[0] // batch_size\n",
    "\n",
    "    ######################\n",
    "    # BUILD ACTUAL MODEL #\n",
    "    ######################\n",
    "    print('... building the model')\n",
    "\n",
    "    # allocate symbolic variables for the data\n",
    "    index = T.lscalar()  # index to a [mini]batch\n",
    "\n",
    "    # generate symbolic variables for input (x and y represent a\n",
    "    # minibatch)\n",
    "    x = T.matrix('x')  # data, presented as rasterized images\n",
    "    y = T.ivector('y')  # labels, presented as 1D vector of [int] labels\n",
    "\n",
    "    # construct the logistic regression class\n",
    "    # Each MNIST image has size 32*32\n",
    "    classifier = LogisticRegression(input=x, n_in=imageSize * imageSize, n_out=numberOfClasses)\n",
    "\n",
    "    # the cost we minimize during training is the negative log likelihood of\n",
    "    # the model in symbolic format\n",
    "    cost = classifier.negative_log_likelihood(y)\n",
    "\n",
    "    # compiling a Theano function that computes the mistakes that are made by\n",
    "    # the model on a minibatch\n",
    "    test_model = theano.function(\n",
    "        inputs=[index],\n",
    "        outputs=classifier.errors(y),\n",
    "        givens={\n",
    "            x: test_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "            y: test_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    validate_model = theano.function(\n",
    "        inputs=[index],\n",
    "        outputs=classifier.errors(y),\n",
    "        givens={\n",
    "            x: valid_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "            y: valid_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # compute the gradient of cost with respect to theta = (W,b)\n",
    "    g_W = T.grad(cost=cost, wrt=classifier.W)\n",
    "    g_b = T.grad(cost=cost, wrt=classifier.b)\n",
    "\n",
    "    # start-snippet-3\n",
    "    # specify how to update the parameters of the model as a list of\n",
    "    # (variable, update expression) pairs.\n",
    "    updates = [(classifier.W, classifier.W - learning_rate * g_W),\n",
    "               (classifier.b, classifier.b - learning_rate * g_b)]\n",
    "\n",
    "    # compiling a Theano function `train_model` that returns the cost, but in\n",
    "    # the same time updates the parameter of the model based on the rules\n",
    "    # defined in `updates`\n",
    "    train_model = theano.function(\n",
    "        inputs=[index],\n",
    "        outputs=cost,\n",
    "        updates=updates,\n",
    "        givens={\n",
    "            x: train_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "            y: train_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "    # end-snippet-3\n",
    "\n",
    "    ###############\n",
    "    # TRAIN MODEL #\n",
    "    ###############\n",
    "    print('... training the model')\n",
    "    # early-stopping parameters\n",
    "    patience = 3000  # look as this many examples regardless\n",
    "    patience_increase = 1  # wait this much longer when a new best is\n",
    "                                  # found\n",
    "    improvement_threshold = 0.995  # a relative improvement of this much is\n",
    "                                  # considered significant\n",
    "    validation_frequency = min(n_train_batches, patience // 2)\n",
    "                                  # go through this many\n",
    "                                  # minibatche before checking the network\n",
    "                                  # on the validation set; in this case we\n",
    "                                  # check every epoch\n",
    "\n",
    "    best_validation_loss = numpy.inf\n",
    "    test_score = 0.\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    done_looping = False\n",
    "    epoch = 0\n",
    "    while (epoch < n_epochs) and (not done_looping):\n",
    "        epoch = epoch + 1\n",
    "        for minibatch_index in range(n_train_batches):\n",
    "            minibatch_avg_cost = train_model(minibatch_index)\n",
    "            # iteration number\n",
    "            iter = (epoch - 1) * n_train_batches + minibatch_index\n",
    "\n",
    "            if (iter + 1) % validation_frequency == 0:\n",
    "                # compute zero-one loss on validation set\n",
    "                validation_losses = [validate_model(i)\n",
    "                                     for i in range(n_valid_batches)]\n",
    "                this_validation_loss = numpy.mean(validation_losses)\n",
    "\n",
    "                print(\n",
    "                    'epoch %i, minibatch %i/%i, validation error %f %%' %\n",
    "                    (\n",
    "                        epoch,\n",
    "                        minibatch_index + 1,\n",
    "                        n_train_batches,\n",
    "                        this_validation_loss * 100.\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                # if we got the best validation score until now\n",
    "                if this_validation_loss < best_validation_loss:\n",
    "                    #improve patience if loss improvement is good enough\n",
    "                    if this_validation_loss < best_validation_loss *  \\\n",
    "                       improvement_threshold:\n",
    "                        patience = max(patience, iter * patience_increase)\n",
    "\n",
    "                    best_validation_loss = this_validation_loss\n",
    "                    # test it on the test set\n",
    "\n",
    "                    test_losses = [test_model(i)\n",
    "                                   for i in range(n_test_batches)]\n",
    "                    test_score = numpy.mean(test_losses)\n",
    "\n",
    "                    print(\n",
    "                        (\n",
    "                            '     epoch %i, minibatch %i/%i, test error of'\n",
    "                            ' best model %f %%'\n",
    "                        ) %\n",
    "                        (\n",
    "                            epoch,\n",
    "                            minibatch_index + 1,\n",
    "                            n_train_batches,\n",
    "                            test_score * 100.\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                    # save the best model\n",
    "                    with open('best_model.pkl', 'wb') as f:\n",
    "                        pickle.dump(classifier, f)\n",
    "\n",
    "            if patience <= iter:\n",
    "                done_looping = True\n",
    "                break\n",
    "\n",
    "    end_time = timeit.default_timer()\n",
    "    print(('Optimization complete with best validation score of %f %%,' 'with test performance %f %%')% (best_validation_loss * 100., test_score * 100.))\n",
    "    #  print('The code run for %d epochs, with %f epochs/sec' % (epoch, 1. * epoch / (end_time - start_time)))\n",
    "    #print(('The code for file ' + os.path.split(__file__)[1] + ' ran for %.1fs' % ((end_time - start_time))), file=sys.stderr)\n",
    "\n",
    "\n",
    "def predict():\n",
    "    \"\"\"\n",
    "    An example of how to load a trained model and use it\n",
    "    to predict labels.\n",
    "    \"\"\"\n",
    "\n",
    "    # load the saved model\n",
    "    classifier = pickle.load(open('best_model.pkl'))\n",
    "\n",
    "    # compile a predictor function\n",
    "    predict_model = theano.function(\n",
    "        inputs=[classifier.input],\n",
    "        outputs=classifier.y_pred)\n",
    "\n",
    "    # We can test it on some examples from test test\n",
    "    \n",
    "    datasets = loadCNNData(trainingSet,testingSet,validationSet)\n",
    "    test_set_x, test_set_y = datasets[2]\n",
    "    test_set_x = test_set_x.get_value()\n",
    "\n",
    "    predicted_values = predict_model(test_set_x[:10])\n",
    "    print(\"Predicted values for the first 10 examples in test set:\")\n",
    "    print(predicted_values)\n",
    "\n",
    "\n",
    "\n",
    "sgd_optimization_mnist(learning_rate=0.9, n_epochs=1000,dataset='mnist.pkl.gz',batch_size=100,imageSize=outputImageSize,numberOfClasses=2)\n",
    "predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(testingSet[1])\n",
    "plt.imshow(segmentationData['testSpheroids'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name LogisticRegression",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-7bfa4129e900>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mlogisticRegression\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mloadCNNData\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mloadCNNData\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name LogisticRegression"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This tutorial introduces the multilayer perceptron using Theano.\n",
    "\n",
    " A multilayer perceptron is a logistic regressor where\n",
    "instead of feeding the input to the logistic regression you insert a\n",
    "intermediate layer, called the hidden layer, that has a nonlinear\n",
    "activation function (usually tanh or sigmoid) . One can use many such\n",
    "hidden layers making the architecture deep. The tutorial will also tackle\n",
    "the problem of MNIST digit classification.\n",
    "\n",
    ".. math::\n",
    "\n",
    "    f(x) = G( b^{(2)} + W^{(2)}( s( b^{(1)} + W^{(1)} x))),\n",
    "\n",
    "References:\n",
    "\n",
    "    - textbooks: \"Pattern Recognition and Machine Learning\" -\n",
    "                 Christopher M. Bishop, section 5\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from logisticRegression import LogisticRegression\n",
    "from loadCNNData import loadCNNData\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import timeit\n",
    "\n",
    "import numpy\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "from hiddenLayer import HiddenLayer\n",
    "\n",
    "\n",
    "\n",
    "# start-snippet-2\n",
    "class MLP(object):\n",
    "    \"\"\"Multi-Layer Perceptron Class\n",
    "\n",
    "    A multilayer perceptron is a feedforward artificial neural network model\n",
    "    that has one layer or more of hidden units and nonlinear activations.\n",
    "    Intermediate layers usually have as activation function tanh or the\n",
    "    sigmoid function (defined here by a ``HiddenLayer`` class)  while the\n",
    "    top layer is a softmax layer (defined here by a ``LogisticRegression``\n",
    "    class).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, rng, input, n_in, n_hidden, n_out):\n",
    "        \"\"\"Initialize the parameters for the multilayer perceptron\n",
    "\n",
    "        :type rng: numpy.random.RandomState\n",
    "        :param rng: a random number generator used to initialize weights\n",
    "\n",
    "        :type input: theano.tensor.TensorType\n",
    "        :param input: symbolic variable that describes the input of the\n",
    "        architecture (one minibatch)\n",
    "\n",
    "        :type n_in: int\n",
    "        :param n_in: number of input units, the dimension of the space in\n",
    "        which the datapoints lie\n",
    "\n",
    "        :type n_hidden: int\n",
    "        :param n_hidden: number of hidden units\n",
    "\n",
    "        :type n_out: int\n",
    "        :param n_out: number of output units, the dimension of the space in\n",
    "        which the labels lie\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Since we are dealing with a one hidden layer MLP, this will translate\n",
    "        # into a HiddenLayer with a tanh activation function connected to the\n",
    "        # LogisticRegression layer; the activation function can be replaced by\n",
    "        # sigmoid or any other nonlinear function\n",
    "        self.hiddenLayer = HiddenLayer(\n",
    "            rng=rng,\n",
    "            input=input,\n",
    "            n_in=n_in,\n",
    "            n_out=n_hidden,\n",
    "            activation=T.tanh\n",
    "        )\n",
    "\n",
    "        # The logistic regression layer gets as input the hidden units\n",
    "        # of the hidden layer\n",
    "        self.logRegressionLayer = LogisticRegression(\n",
    "            input=self.hiddenLayer.output,\n",
    "            n_in=n_hidden,\n",
    "            n_out=n_out\n",
    "        )\n",
    "        # end-snippet-2 start-snippet-3\n",
    "        # L1 norm ; one regularization option is to enforce L1 norm to\n",
    "        # be small\n",
    "        self.L1 = (\n",
    "            abs(self.hiddenLayer.W).sum()\n",
    "            + abs(self.logRegressionLayer.W).sum()\n",
    "        )\n",
    "\n",
    "        # square of L2 norm ; one regularization option is to enforce\n",
    "        # square of L2 norm to be small\n",
    "        self.L2_sqr = (\n",
    "            (self.hiddenLayer.W ** 2).sum()\n",
    "            + (self.logRegressionLayer.W ** 2).sum()\n",
    "        )\n",
    "\n",
    "        # negative log likelihood of the MLP is given by the negative\n",
    "        # log likelihood of the output of the model, computed in the\n",
    "        # logistic regression layer\n",
    "        self.negative_log_likelihood = (\n",
    "            self.logRegressionLayer.negative_log_likelihood\n",
    "        )\n",
    "        # same holds for the function computing the number of errors\n",
    "        self.errors = self.logRegressionLayer.errors\n",
    "\n",
    "        # the parameters of the model are the parameters of the two layer it is\n",
    "        # made out of\n",
    "        self.params = self.hiddenLayer.params + self.logRegressionLayer.params\n",
    "        # end-snippet-3\n",
    "\n",
    "        # keep track of model input\n",
    "        self.input = input\n",
    "\n",
    "\n",
    "def test_mlp(learning_rate=0.9, L1_reg=0.00, L2_reg=0.0001, n_epochs=1000,\n",
    "             dataset='mnist.pkl.gz', batch_size=100, n_hidden=500):\n",
    "    \"\"\"\n",
    "    Demonstrate stochastic gradient descent optimization for a multilayer\n",
    "    perceptron\n",
    "\n",
    "    This is demonstrated on MNIST.\n",
    "\n",
    "    :type learning_rate: float\n",
    "    :param learning_rate: learning rate used (factor for the stochastic\n",
    "    gradient\n",
    "\n",
    "    :type L1_reg: float\n",
    "    :param L1_reg: L1-norm's weight when added to the cost (see\n",
    "    regularization)\n",
    "\n",
    "    :type L2_reg: float\n",
    "    :param L2_reg: L2-norm's weight when added to the cost (see\n",
    "    regularization)\n",
    "\n",
    "    :type n_epochs: int\n",
    "    :param n_epochs: maximal number of epochs to run the optimizer\n",
    "\n",
    "    :type dataset: string\n",
    "    :param dataset: the path of the MNIST dataset file from\n",
    "                 http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz\n",
    "\n",
    "\n",
    "   \"\"\"\n",
    "    datasets = loadCNNData(trainingSet,testingSet,validationSet)\n",
    "\n",
    "    train_set_x, train_set_y = datasets[0]\n",
    "    valid_set_x, valid_set_y = datasets[1]\n",
    "    test_set_x, test_set_y = datasets[2]\n",
    "\n",
    "    # compute number of minibatches for training, validation and testing\n",
    "    n_train_batches = train_set_x.get_value(borrow=True).shape[0] // batch_size\n",
    "    n_valid_batches = valid_set_x.get_value(borrow=True).shape[0] // batch_size\n",
    "    n_test_batches = test_set_x.get_value(borrow=True).shape[0] // batch_size\n",
    "\n",
    "    ######################\n",
    "    # BUILD ACTUAL MODEL #\n",
    "    ######################\n",
    "    print('... building the model')\n",
    "\n",
    "    # allocate symbolic variables for the data\n",
    "    index = T.lscalar()  # index to a [mini]batch\n",
    "    x = T.matrix('x')  # the data is presented as rasterized images\n",
    "    y = T.ivector('y')  # the labels are presented as 1D vector of\n",
    "                        # [int] labels\n",
    "\n",
    "    rng = numpy.random.RandomState(1234)\n",
    "\n",
    "    # construct the MLP class\n",
    "    classifier = MLP(\n",
    "        rng=rng,\n",
    "        input=x,\n",
    "        n_in=128 * 128,\n",
    "        n_hidden=n_hidden,\n",
    "        n_out=2\n",
    "    )\n",
    "\n",
    "    # start-snippet-4\n",
    "    # the cost we minimize during training is the negative log likelihood of\n",
    "    # the model plus the regularization terms (L1 and L2); cost is expressed\n",
    "    # here symbolically\n",
    "    cost = (\n",
    "        classifier.negative_log_likelihood(y)\n",
    "        + L1_reg * classifier.L1\n",
    "        + L2_reg * classifier.L2_sqr\n",
    "    )\n",
    "    # end-snippet-4\n",
    "\n",
    "    # compiling a Theano function that computes the mistakes that are made\n",
    "    # by the model on a minibatch\n",
    "    test_model = theano.function(\n",
    "        inputs=[index],\n",
    "        outputs=classifier.errors(y),\n",
    "        givens={\n",
    "            x: test_set_x[index * batch_size:(index + 1) * batch_size],\n",
    "            y: test_set_y[index * batch_size:(index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    validate_model = theano.function(\n",
    "        inputs=[index],\n",
    "        outputs=classifier.errors(y),\n",
    "        givens={\n",
    "            x: valid_set_x[index * batch_size:(index + 1) * batch_size],\n",
    "            y: valid_set_y[index * batch_size:(index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # start-snippet-5\n",
    "    # compute the gradient of cost with respect to theta (sorted in params)\n",
    "    # the resulting gradients will be stored in a list gparams\n",
    "    gparams = [T.grad(cost, param) for param in classifier.params]\n",
    "\n",
    "    # specify how to update the parameters of the model as a list of\n",
    "    # (variable, update expression) pairs\n",
    "\n",
    "    # given two lists of the same length, A = [a1, a2, a3, a4] and\n",
    "    # B = [b1, b2, b3, b4], zip generates a list C of same size, where each\n",
    "    # element is a pair formed from the two lists :\n",
    "    #    C = [(a1, b1), (a2, b2), (a3, b3), (a4, b4)]\n",
    "    updates = [\n",
    "        (param, param - learning_rate * gparam)\n",
    "        for param, gparam in zip(classifier.params, gparams)\n",
    "    ]\n",
    "\n",
    "    # compiling a Theano function `train_model` that returns the cost, but\n",
    "    # in the same time updates the parameter of the model based on the rules\n",
    "    # defined in `updates`\n",
    "    train_model = theano.function(\n",
    "        inputs=[index],\n",
    "        outputs=cost,\n",
    "        updates=updates,\n",
    "        givens={\n",
    "            x: train_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "            y: train_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "    # end-snippet-5\n",
    "\n",
    "    ###############\n",
    "    # TRAIN MODEL #\n",
    "    ###############\n",
    "    print('... training')\n",
    "\n",
    "    # early-stopping parameters\n",
    "    patience = 1000  # look as this many examples regardless\n",
    "    patience_increase = 2  # wait this much longer when a new best is\n",
    "                           # found\n",
    "    improvement_threshold = 0.995  # a relative improvement of this much is\n",
    "                                   # considered significant\n",
    "    validation_frequency = min(n_train_batches, patience // 2)\n",
    "                                  # go through this many\n",
    "                                  # minibatche before checking the network\n",
    "                                  # on the validation set; in this case we\n",
    "                                  # check every epoch\n",
    "\n",
    "    best_validation_loss = numpy.inf\n",
    "    best_iter = 0\n",
    "    test_score = 0.\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    epoch = 0\n",
    "    done_looping = False\n",
    "\n",
    "    while (epoch < n_epochs) and (not done_looping):\n",
    "        epoch = epoch + 1\n",
    "        for minibatch_index in range(n_train_batches):\n",
    "\n",
    "            minibatch_avg_cost = train_model(minibatch_index)\n",
    "            # iteration number\n",
    "            iter = (epoch - 1) * n_train_batches + minibatch_index\n",
    "\n",
    "            if (iter + 1) % validation_frequency == 0:\n",
    "                # compute zero-one loss on validation set\n",
    "                validation_losses = [validate_model(i) for i\n",
    "                                     in range(n_valid_batches)]\n",
    "                this_validation_loss = numpy.mean(validation_losses)\n",
    "\n",
    "                print(\n",
    "                    'epoch %i, minibatch %i/%i, validation error %f %%' %\n",
    "                    (\n",
    "                        epoch,\n",
    "                        minibatch_index + 1,\n",
    "                        n_train_batches,\n",
    "                        this_validation_loss * 100.\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                # if we got the best validation score until now\n",
    "                if this_validation_loss < best_validation_loss:\n",
    "                    #improve patience if loss improvement is good enough\n",
    "                    if (\n",
    "                        this_validation_loss < best_validation_loss *\n",
    "                        improvement_threshold\n",
    "                    ):\n",
    "                        patience = max(patience, iter * patience_increase)\n",
    "\n",
    "                    best_validation_loss = this_validation_loss\n",
    "                    best_iter = iter\n",
    "\n",
    "                    # test it on the test set\n",
    "                    test_losses = [test_model(i) for i\n",
    "                                   in range(n_test_batches)]\n",
    "                    test_score = numpy.mean(test_losses)\n",
    "\n",
    "                    print(('     epoch %i, minibatch %i/%i, test error of '\n",
    "                           'best model %f %%') %\n",
    "                          (epoch, minibatch_index + 1, n_train_batches,\n",
    "                           test_score * 100.))\n",
    "\n",
    "            if patience <= iter:\n",
    "                done_looping = True\n",
    "                break\n",
    "\n",
    "    end_time = timeit.default_timer()\n",
    "    print(('Optimization complete. Best validation score of %f %% '\n",
    "           'obtained at iteration %i, with test performance %f %%') %\n",
    "          (best_validation_loss * 100., best_iter + 1, test_score * 100.))\n",
    "    print('The code for file ' +\n",
    "           'MLP' +\n",
    "           ' ran for %.2fm' % ((end_time - start_time) / 60.))\n",
    "\n",
    "\n",
    "\n",
    "test_mlp()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-7fb7e987fe86>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-3-7fb7e987fe86>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    var cell = Jupyter.notebook.get_selected_cell();\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
