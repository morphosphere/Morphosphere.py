{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cPickle as pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob,os\n",
    "import cv2\n",
    "\n",
    "#theano\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import timeit\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline  \n",
    "\n",
    "from segmentSpheroid import segmentSpheroid\n",
    "from generateBatch import generateBatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z:\\Vardan_Andriasyan\\Morphosphere\\2classes\\\\Test\\NonSpheroid\\20160126-corning-all-spheroids-p4-006hps_D05_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\2classes\\\\Test\\NonSpheroid\\20160126-corning-all-spheroids-p4-006hps_F05_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\2classes\\\\Test\\NonSpheroid\\20160126-corning-all-spheroids-p4-006hps_I03_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\2classes\\\\Test\\NonSpheroid\\20160126-corning-all-spheroids-p5-007hps_M01_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\2classes\\\\Test\\NonSpheroid\\20160127-corning-all-spheroids-p1-020hps_B05_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\2classes\\\\Test\\Spheroid\\20160130-corning-all-spheroids-p5-098hps_F06_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\2classes\\\\Test\\Spheroid\\20160130-corning-all-spheroids-p2-095hps_O05_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\2classes\\\\Test\\Spheroid\\20160130-corning-all-spheroids-p5-098hps_E01_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\2classes\\\\Test\\Spheroid\\20160130-corning-all-spheroids-p5-098hps_E06_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\2classes\\\\Test\\Spheroid\\20160130-corning-all-spheroids-p5-098hps_F01_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\2classes\\\\Training\\NonSpheroid\\20160126-corning-all-spheroids-p4-006hps_E02_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\2classes\\\\Training\\NonSpheroid\\20160126-corning-all-spheroids-p4-006hps_G05_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\2classes\\\\Training\\NonSpheroid\\20160126-corning-all-spheroids-p2-004hps_F01_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\2classes\\\\Training\\NonSpheroid\\20160126-corning-all-spheroids-p2-004hps_C06_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\2classes\\\\Training\\NonSpheroid\\20160126-corning-all-spheroids-p2-004hps_E03_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\2classes\\\\Training\\Spheroid\\20160129-corning-all-spheroids-p4-072hps_M03_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\2classes\\\\Training\\Spheroid\\20160131-corning-all-spheroids-p4-122hps_B06_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\2classes\\\\Training\\Spheroid\\20160128-corning-all-spheroids-p3-050hps_H05_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\2classes\\\\Training\\Spheroid\\20160128-corning-all-spheroids-p3-050hps_K06_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\2classes\\\\Training\\Spheroid\\20160129-corning-all-spheroids-p2-070hps_O05_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\2classes\\\\Validation\\NonSpheroid\\20160126-corning-all-spheroids-p4-006hps_E06_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\2classes\\\\Validation\\NonSpheroid\\20160126-corning-all-spheroids-p2-004hps_K02_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\2classes\\\\Validation\\NonSpheroid\\20160126-corning-all-spheroids-p4-006hps_C05_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\2classes\\\\Validation\\NonSpheroid\\20160126-corning-all-spheroids-p2-004hps_I04_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\2classes\\\\Validation\\NonSpheroid\\20160126-corning-all-spheroids-p2-004hps_I04_w1 - Copy.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\2classes\\\\Validation\\Spheroid\\20160130-corning-all-spheroids-p5-098hps_C06_w3.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\2classes\\\\Validation\\Spheroid\\20160130-corning-all-spheroids-p5-098hps_D06_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\2classes\\\\Validation\\Spheroid\\20160130-corning-all-spheroids-p1-094hps_O04_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\2classes\\\\Validation\\Spheroid\\20160129-corning-all-spheroids-p1-069hps_L06_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\2classes\\\\Validation\\Spheroid\\20160128-corning-all-spheroids-p3-050hps_J05_w1.TIF\n",
      "878\n"
     ]
    }
   ],
   "source": [
    "##############################################\n",
    "#################2 CLASSES####################\n",
    "##############################################\n",
    "\"\"\"\n",
    "Demo for Morphosphere\n",
    "\n",
    "@author: Vardan Andriasyan\n",
    "\"\"\"\n",
    "\n",
    "#\n",
    "parentDir = 'Z:\\\\Vardan_Andriasyan\\\\Morphosphere\\\\2classes\\\\'\n",
    "sets = ['Test','Training','Validation']#'Test','Training','Validation'\n",
    "classes = ['NonSpheroid','Spheroid']#'NonSphHealthy','NonSphNonHealthy','SphHealthy','SphNonHealthy'\n",
    "\n",
    "#inputImage= 'Z:\\\\Vardan_Andriasyan\\\\Morphosphere\\\\HT-29\\\\Test\\\\NonSphNonHealthy\\\\20160205-corning-all-spheroids-p4-242hps_G05_w1.TIF'\n",
    "#fullImage, croppedImage, croppedBWImage, spheroidAttributes = segmentSpheroid(inputImage, 12, 501, 20000)\n",
    "\n",
    "trainingSpheroids = []\n",
    "testSpheroids = []\n",
    "validationSpheroids = []\n",
    "trainingLabels = []\n",
    "testLabels = []\n",
    "validationLabels = []\n",
    "\n",
    "allSizes = []\n",
    "\n",
    "for iSet in sets:\n",
    "    for iClass in classes:\n",
    "        inputFolderPath = parentDir+'\\\\'+iSet+'\\\\'+iClass\n",
    "        for currentImagePath in glob.glob(inputFolderPath+'\\\\*.TIF'):\n",
    "            print(currentImagePath)\n",
    "            croppedImage, croppedBWImage, spheroidAttributes = segmentSpheroid(currentImagePath, 20, 501, 20000)\n",
    "            #populate an array with all image sizes\n",
    "            imageHeight, imageWidth = croppedImage.shape[:2]\n",
    "            allSizes.append(max([imageHeight,imageWidth]))\n",
    "            if iSet == 'Test':\n",
    "                testSpheroids.append(croppedImage)\n",
    "                if iClass == 'NonSpheroid':\n",
    "                    testLabels.append([0])\n",
    "                if iClass == 'Spheroid':\n",
    "                    testLabels.append([1])\n",
    "            if iSet == 'Training':\n",
    "                trainingSpheroids.append(croppedImage)\n",
    "                if iClass == 'NonSpheroid':\n",
    "                    trainingLabels.append([0])\n",
    "                if iClass == 'Spheroid':\n",
    "                    trainingLabels.append([1])\n",
    "            if iSet == 'Validation':\n",
    "                validationLabels.append(croppedImage)\n",
    "                if iClass == 'NonSpheroid':\n",
    "                    validationLabels.append([0])\n",
    "                if iClass == 'Spheroid':\n",
    "                    validationLabels.append([1])\n",
    "             \n",
    "\n",
    "maximumImageSize =  max(allSizes)\n",
    "print(max(allSizes))          \n",
    "segmentationData = { \"testSpheroids\": testSpheroids, \"testLabels\": testLabels,\"trainingSpheroids\": testSpheroids, \"trainingLabels\": testLabels,\"validationSpheroids\": testSpheroids, \"validationLabels\": testLabels,\"maximumImageSize\": maximumImageSize}\n",
    "pickle.dump( segmentationData, open( \"segmentationData2Classes.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphHealthy\\20160126-corning-all-spheroids-p5-007hps_M01_w1.TIF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "segmentSpheroid.py:54: VisibleDeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future\n",
      "  selectedCC = (labeledImage == allProperties[indexOfMinDistance].label)\n",
      "segmentSpheroid.py:58: VisibleDeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future\n",
      "  boundingBox =  allProperties[indexOfMinDistance].bbox\n",
      "segmentSpheroid.py:61: VisibleDeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future\n",
      "  centroid = allProperties[indexOfMinDistance].centroid\n",
      "segmentSpheroid.py:62: VisibleDeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future\n",
      "  perimeter = allProperties[indexOfMinDistance].perimeter\n",
      "segmentSpheroid.py:63: VisibleDeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future\n",
      "  area = allProperties[indexOfMinDistance].area\n",
      "segmentSpheroid.py:64: VisibleDeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future\n",
      "  diameter = allProperties[indexOfMinDistance].equivalent_diameter\n",
      "segmentSpheroid.py:65: VisibleDeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future\n",
      "  majorAxis = allProperties[indexOfMinDistance].major_axis_length\n",
      "segmentSpheroid.py:66: VisibleDeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future\n",
      "  minorAxis = allProperties[indexOfMinDistance].minor_axis_length\n",
      "segmentSpheroid.py:78: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  croppedBWImage = spheroidBWImage[minRow:maxRow,minCol:maxCol]\n",
      "segmentSpheroid.py:79: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  croppedImage  =  inputImage[minRow:maxRow,minCol:maxCol]*croppedBWImage\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphHealthy\\20160127-corning-all-spheroids-p1-020hps_L01_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphHealthy\\20160127-corning-all-spheroids-p1-020hps_H06_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphHealthy\\20160127-corning-all-spheroids-p1-020hps_F05_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphHealthy\\20160127-corning-all-spheroids-p1-020hps_D03_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphHealthy\\20160127-corning-all-spheroids-p1-020hps_B05_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphHealthy\\20160126-corning-all-spheroids-p4-006hps_D05_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphHealthy\\20160126-corning-all-spheroids-p4-006hps_F05_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphHealthy\\20160126-corning-all-spheroids-p4-006hps_I03_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphHealthy\\20160126-corning-all-spheroids-p2-004hps_D03_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphHealthy\\20160126-corning-all-spheroids-p2-004hps_E06_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphHealthy\\20160126-corning-all-spheroids-p2-004hps_F04_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphHealthy\\20160126-corning-all-spheroids-p2-004hps_G03_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphHealthy\\20160126-corning-all-spheroids-p2-004hps_H01_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphHealthy\\20160126-corning-all-spheroids-p2-004hps_H06_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphHealthy\\20160127-corning-all-spheroids-p1-020hps_D05_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphHealthy\\20160127-corning-all-spheroids-p1-020hps_E03_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphHealthy\\20160127-corning-all-spheroids-p1-020hps_G03_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphHealthy\\20160127-corning-all-spheroids-p1-020hps_O04_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphHealthy\\20160127-corning-all-spheroids-p1-020hps_O02_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphNonHealthy\\20160205-corning-all-spheroids-p4-242hps_F05_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphNonHealthy\\20160205-corning-all-spheroids-p4-242hps_G02_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphNonHealthy\\20160205-corning-all-spheroids-p4-242hps_G04_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphNonHealthy\\20160205-corning-all-spheroids-p4-242hps_G05_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphNonHealthy\\20160205-corning-all-spheroids-p4-242hps_H02_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphNonHealthy\\20160205-corning-all-spheroids-p4-242hps_H03_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphNonHealthy\\20160205-corning-all-spheroids-p4-242hps_H05_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphNonHealthy\\20160205-corning-all-spheroids-p4-242hps_I02_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphNonHealthy\\20160205-corning-all-spheroids-p4-242hps_I03_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphNonHealthy\\20160205-corning-all-spheroids-p4-242hps_I04_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphNonHealthy\\20160204-corning-all-spheroids-p4-218hps_G05_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphNonHealthy\\20160204-corning-all-spheroids-p4-218hps_H02_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphNonHealthy\\20160204-corning-all-spheroids-p4-218hps_I02_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphNonHealthy\\20160204-corning-all-spheroids-p4-218hps_I03_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphNonHealthy\\20160204-corning-all-spheroids-p4-218hps_I04_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphNonHealthy\\20160204-corning-all-spheroids-p4-218hps_K04_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphNonHealthy\\20160204-corning-all-spheroids-p4-218hps_M04_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphNonHealthy\\20160204-corning-all-spheroids-p4-218hps_L02_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphNonHealthy\\20160204-corning-all-spheroids-p4-218hps_L03_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\NonSphNonHealthy\\20160204-corning-all-spheroids-p4-218hps_L04_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\SphHealthy\\20160129-corning-all-spheroids-p4-072hps_N05_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\SphHealthy\\20160130-corning-all-spheroids-p4-097hps_M03_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\SphHealthy\\20160205-corning-all-spheroids-p1-237hps_L06_w4.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\SphHealthy\\20160129-corning-all-spheroids-p1-069hps_O04_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\SphHealthy\\20160130-corning-all-spheroids-p2-095hps_O05_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\SphHealthy\\20160128-corning-all-spheroids-p3-050hps_K04_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\SphHealthy\\20160128-corning-all-spheroids-p3-050hps_M03_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\SphHealthy\\20160205-corning-all-spheroids-p4-242hps_G01_w5.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\SphHealthy\\20160205-corning-all-spheroids-p4-242hps_H01_w5.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\SphHealthy\\20160130-corning-all-spheroids-p5-098hps_E01_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\SphHealthy\\20160130-corning-all-spheroids-p5-098hps_E06_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\SphHealthy\\20160130-corning-all-spheroids-p5-098hps_F01_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\SphHealthy\\20160130-corning-all-spheroids-p5-098hps_F06_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\SphHealthy\\20160131-corning-all-spheroids-p5-124hps_C06_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\SphHealthy\\20160131-corning-all-spheroids-p5-124hps_D01_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\SphHealthy\\20160131-corning-all-spheroids-p5-124hps_D06_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\SphHealthy\\20160131-corning-all-spheroids-p5-124hps_E01_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\SphHealthy\\20160202-corning-all-spheroids-p5-171hps_N06_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\SphHealthy\\20160202-corning-all-spheroids-p5-171hps_O06_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\SphHealthy\\20160203-corning-all-spheroids-p5-196hps_G06_w4.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\SphNonHealthy\\20160201-corning-all-spheroids-p4-146hps_N05_w5.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\SphNonHealthy\\20160205-corning-all-spheroids-p2-238hps_O05_w5.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\SphNonHealthy\\20160130-corning-all-spheroids-p5-098hps_E02_w3.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\SphNonHealthy\\20160130-corning-all-spheroids-p5-098hps_E03_w3.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\SphNonHealthy\\20160130-corning-all-spheroids-p5-098hps_E05_w3.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\SphNonHealthy\\20160130-corning-all-spheroids-p5-098hps_F02_w3.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\SphNonHealthy\\20160130-corning-all-spheroids-p5-098hps_F03_w3.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\SphNonHealthy\\20160130-corning-all-spheroids-p5-098hps_F04_w3.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\SphNonHealthy\\20160130-corning-all-spheroids-p5-098hps_F05_w3.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\SphNonHealthy\\20160131-corning-all-spheroids-p5-124hps_C02_w3.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\SphNonHealthy\\20160131-corning-all-spheroids-p5-124hps_C03_w3.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\SphNonHealthy\\20160131-corning-all-spheroids-p5-124hps_C04_w3.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\SphNonHealthy\\20160131-corning-all-spheroids-p5-124hps_C05_w3.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\SphNonHealthy\\20160131-corning-all-spheroids-p5-124hps_D02_w3.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\SphNonHealthy\\20160131-corning-all-spheroids-p5-124hps_D03_w3.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\SphNonHealthy\\20160131-corning-all-spheroids-p5-124hps_D04_w3.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\SphNonHealthy\\20160131-corning-all-spheroids-p5-124hps_D05_w3.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\SphNonHealthy\\20160131-corning-all-spheroids-p5-124hps_E02_w3.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Test\\SphNonHealthy\\20160202-corning-all-spheroids-p5-171hps_N02_w3.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Training\\NonSphHealthy\\20160126-corning-all-spheroids-p5-007hps_A03_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Training\\NonSphHealthy\\20160127-corning-all-spheroids-p1-020hps_P02_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Training\\NonSphHealthy\\20160127-corning-all-spheroids-p1-020hps_K01_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Training\\NonSphHealthy\\20160127-corning-all-spheroids-p1-020hps_H02_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Training\\NonSphHealthy\\20160127-corning-all-spheroids-p1-020hps_F01_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Training\\NonSphHealthy\\20160127-corning-all-spheroids-p1-020hps_D06_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Training\\NonSphHealthy\\20160126-corning-all-spheroids-p4-006hps_B06_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Training\\NonSphHealthy\\20160126-corning-all-spheroids-p4-006hps_E02_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Training\\NonSphHealthy\\20160126-corning-all-spheroids-p4-006hps_G05_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Training\\NonSphHealthy\\20160126-corning-all-spheroids-p2-004hps_C04_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Training\\NonSphNonHealthy\\20160205-corning-all-spheroids-p4-242hps_M03_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Training\\NonSphNonHealthy\\20160205-corning-all-spheroids-p4-242hps_A02_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Training\\NonSphNonHealthy\\20160205-corning-all-spheroids-p4-242hps_A03_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Training\\NonSphNonHealthy\\20160205-corning-all-spheroids-p4-242hps_A04_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Training\\NonSphNonHealthy\\20160205-corning-all-spheroids-p4-242hps_A05_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Training\\NonSphNonHealthy\\20160204-corning-all-spheroids-p4-218hps_A02_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Training\\NonSphNonHealthy\\20160204-corning-all-spheroids-p4-218hps_P05_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Training\\NonSphNonHealthy\\20160204-corning-all-spheroids-p4-218hps_O05_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Training\\NonSphNonHealthy\\20160204-corning-all-spheroids-p4-218hps_O02_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Training\\NonSphNonHealthy\\20160204-corning-all-spheroids-p4-218hps_N04_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Training\\SphHealthy\\20160129-corning-all-spheroids-p4-072hps_M03_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Training\\SphHealthy\\20160130-corning-all-spheroids-p1-094hps_L06_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Training\\SphHealthy\\20160129-corning-all-spheroids-p2-070hps_O05_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Training\\SphHealthy\\20160128-corning-all-spheroids-p3-050hps_H05_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Training\\SphHealthy\\20160128-corning-all-spheroids-p3-050hps_K06_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Training\\SphHealthy\\20160205-corning-all-spheroids-p4-242hps_A06_w5.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Training\\SphHealthy\\20160205-corning-all-spheroids-p4-242hps_D01_w5.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Training\\SphHealthy\\20160131-corning-all-spheroids-p4-122hps_B06_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Training\\SphHealthy\\20160202-corning-all-spheroids-p4-170hps_E06_w5.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Training\\SphHealthy\\20160202-corning-all-spheroids-p4-170hps_P06_w5.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Training\\SphNonHealthy\\20160201-corning-all-spheroids-p1-141hps_O04_w4.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Training\\SphNonHealthy\\20160202-corning-all-spheroids-p2-166hps_O05_w3.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Training\\SphNonHealthy\\20160201-corning-all-spheroids-p4-146hps_A02_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Training\\SphNonHealthy\\20160130-corning-all-spheroids-p5-098hps_A02_w3.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Training\\SphNonHealthy\\20160130-corning-all-spheroids-p5-098hps_A04_w3.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Training\\SphNonHealthy\\20160131-corning-all-spheroids-p5-124hps_A03_w3.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Training\\SphNonHealthy\\20160131-corning-all-spheroids-p5-124hps_A04_w3.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Training\\SphNonHealthy\\20160201-corning-all-spheroids-p5-149hps_A02_w3.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Training\\SphNonHealthy\\20160202-corning-all-spheroids-p5-171hps_B02_w3.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphHealthy\\20160126-corning-all-spheroids-p5-007hps_E05_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphHealthy\\20160127-corning-all-spheroids-p1-020hps_N04_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphHealthy\\20160127-corning-all-spheroids-p1-020hps_I06_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphHealthy\\20160127-corning-all-spheroids-p1-020hps_G04_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphHealthy\\20160127-corning-all-spheroids-p1-020hps_E05_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphHealthy\\20160127-corning-all-spheroids-p1-020hps_C02_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphHealthy\\20160126-corning-all-spheroids-p4-006hps_C05_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphHealthy\\20160126-corning-all-spheroids-p4-006hps_E06_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphHealthy\\20160126-corning-all-spheroids-p4-006hps_H04_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphHealthy\\20160126-corning-all-spheroids-p2-004hps_C06_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphHealthy\\20160126-corning-all-spheroids-p2-004hps_E03_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphHealthy\\20160126-corning-all-spheroids-p2-004hps_F01_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphHealthy\\20160126-corning-all-spheroids-p2-004hps_I03_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphHealthy\\20160126-corning-all-spheroids-p2-004hps_I04_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphHealthy\\20160126-corning-all-spheroids-p2-004hps_K02_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphHealthy\\20160127-corning-all-spheroids-p1-020hps_A04_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphHealthy\\20160127-corning-all-spheroids-p1-020hps_A06_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphHealthy\\20160127-corning-all-spheroids-p1-020hps_B03_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphHealthy\\20160127-corning-all-spheroids-p1-020hps_C03_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphHealthy\\20160127-corning-all-spheroids-p1-020hps_C05_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphNonHealthy\\20160205-corning-all-spheroids-p4-242hps_N05_w3.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphNonHealthy\\20160205-corning-all-spheroids-p4-242hps_B02_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphNonHealthy\\20160205-corning-all-spheroids-p4-242hps_B03_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphNonHealthy\\20160205-corning-all-spheroids-p4-242hps_B04_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphNonHealthy\\20160205-corning-all-spheroids-p4-242hps_C02_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphNonHealthy\\20160205-corning-all-spheroids-p4-242hps_C04_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphNonHealthy\\20160205-corning-all-spheroids-p4-242hps_C05_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphNonHealthy\\20160205-corning-all-spheroids-p4-242hps_E03_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphNonHealthy\\20160205-corning-all-spheroids-p4-242hps_F03_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphNonHealthy\\20160205-corning-all-spheroids-p4-242hps_F04_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphNonHealthy\\20160204-corning-all-spheroids-p4-218hps_B02_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphNonHealthy\\20160204-corning-all-spheroids-p4-218hps_B03_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphNonHealthy\\20160204-corning-all-spheroids-p4-218hps_B05_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphNonHealthy\\20160204-corning-all-spheroids-p4-218hps_C05_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphNonHealthy\\20160204-corning-all-spheroids-p4-218hps_D05_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphNonHealthy\\20160204-corning-all-spheroids-p4-218hps_E03_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphNonHealthy\\20160204-corning-all-spheroids-p4-218hps_F03_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphNonHealthy\\20160204-corning-all-spheroids-p4-218hps_F04_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphNonHealthy\\20160204-corning-all-spheroids-p4-218hps_G02_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\NonSphNonHealthy\\20160204-corning-all-spheroids-p4-218hps_G04_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphHealthy\\20160130-corning-all-spheroids-p4-097hps_N05_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphHealthy\\20160129-corning-all-spheroids-p1-069hps_L06_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphHealthy\\20160201-corning-all-spheroids-p1-141hps_L06_w4.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphHealthy\\20160130-corning-all-spheroids-p1-094hps_O04_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphHealthy\\20160128-corning-all-spheroids-p3-050hps_J05_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphHealthy\\20160128-corning-all-spheroids-p3-050hps_L03_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphHealthy\\20160205-corning-all-spheroids-p4-242hps_B06_w5.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphHealthy\\20160205-corning-all-spheroids-p4-242hps_C06_w5.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphHealthy\\20160130-corning-all-spheroids-p5-098hps_C06_w3.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphHealthy\\20160130-corning-all-spheroids-p5-098hps_D01_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphHealthy\\20160130-corning-all-spheroids-p5-098hps_D06_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphHealthy\\20160203-corning-all-spheroids-p5-196hps_H06_w4.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphHealthy\\20160203-corning-all-spheroids-p5-196hps_H01_w4.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphHealthy\\20160202-corning-all-spheroids-p5-171hps_M01_w4.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphHealthy\\20160202-corning-all-spheroids-p5-171hps_M06_w4.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphHealthy\\20160202-corning-all-spheroids-p5-171hps_L01_w4.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphHealthy\\20160201-corning-all-spheroids-p5-149hps_D01_w4.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphHealthy\\20160201-corning-all-spheroids-p5-149hps_D06_w4.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphHealthy\\20160201-corning-all-spheroids-p5-149hps_E01_w4.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphHealthy\\20160131-corning-all-spheroids-p5-124hps_A01_w1.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphNonHealthy\\20160201-corning-all-spheroids-p4-146hps_M03_w5.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphNonHealthy\\20160205-corning-all-spheroids-p1-237hps_O04_w4.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphNonHealthy\\20160130-corning-all-spheroids-p5-098hps_C02_w3.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphNonHealthy\\20160130-corning-all-spheroids-p5-098hps_C03_w3.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphNonHealthy\\20160130-corning-all-spheroids-p5-098hps_C04_w3.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphNonHealthy\\20160130-corning-all-spheroids-p5-098hps_C05_w3.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphNonHealthy\\20160130-corning-all-spheroids-p5-098hps_D02_w3.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphNonHealthy\\20160130-corning-all-spheroids-p5-098hps_D03_w3.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphNonHealthy\\20160130-corning-all-spheroids-p5-098hps_D04_w3.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphNonHealthy\\20160130-corning-all-spheroids-p5-098hps_D05_w3.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphNonHealthy\\20160203-corning-all-spheroids-p5-196hps_H05_w3.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphNonHealthy\\20160203-corning-all-spheroids-p5-196hps_H04_w3.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphNonHealthy\\20160203-corning-all-spheroids-p5-196hps_H02_w4.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphNonHealthy\\20160202-corning-all-spheroids-p5-171hps_N03_w3.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphNonHealthy\\20160202-corning-all-spheroids-p5-171hps_N05_w3.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphNonHealthy\\20160202-corning-all-spheroids-p5-171hps_M03_w3.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphNonHealthy\\20160202-corning-all-spheroids-p5-171hps_M04_w3.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphNonHealthy\\20160202-corning-all-spheroids-p5-171hps_L02_w3.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphNonHealthy\\20160201-corning-all-spheroids-p5-149hps_D02_w3.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphNonHealthy\\20160201-corning-all-spheroids-p5-149hps_D03_w3.TIF\n",
      "Z:\\Vardan_Andriasyan\\Morphosphere\\HT-29\\\\Validation\\SphNonHealthy\\20160201-corning-all-spheroids-p5-149hps_D04_w3.TIF\n",
      "1358\n"
     ]
    }
   ],
   "source": [
    "##############################################\n",
    "#################4 CLASSES####################\n",
    "##############################################\n",
    "\"\"\"\n",
    "Demo for Morphosphere\n",
    "\n",
    "@author: Vardan Andriasyan\n",
    "\"\"\"\n",
    "\n",
    "#\n",
    "parentDir = 'Z:\\\\Vardan_Andriasyan\\\\Morphosphere\\\\HT-29\\\\'\n",
    "sets = ['Test','Training','Validation']#'Test','Training','Validation'\n",
    "classes = ['NonSphHealthy','NonSphNonHealthy','SphHealthy','SphNonHealthy']#'NonSphHealthy','NonSphNonHealthy','SphHealthy','SphNonHealthy'\n",
    "\n",
    "#inputImage= 'Z:\\\\Vardan_Andriasyan\\\\Morphosphere\\\\HT-29\\\\Test\\\\NonSphNonHealthy\\\\20160205-corning-all-spheroids-p4-242hps_G05_w1.TIF'\n",
    "#fullImage, croppedImage, croppedBWImage, spheroidAttributes = segmentSpheroid(inputImage, 12, 501, 20000)\n",
    "\n",
    "trainingSpheroids = []\n",
    "testSpheroids = []\n",
    "validationSpheroids = []\n",
    "trainingLabels = []\n",
    "testLabels = []\n",
    "validationLabels = []\n",
    "\n",
    "allSizes = []\n",
    "\n",
    "for iSet in sets:\n",
    "    for iClass in classes:\n",
    "        inputFolderPath = parentDir+'\\\\'+iSet+'\\\\'+iClass\n",
    "        for currentImagePath in glob.glob(inputFolderPath+'\\\\*.TIF'):\n",
    "            print(currentImagePath)\n",
    "            croppedImage, croppedBWImage, spheroidAttributes = segmentSpheroid(currentImagePath, 20, 501, 20000)\n",
    "            #populate an array with all image sizes\n",
    "            imageHeight, imageWidth = croppedImage.shape[:2]\n",
    "            allSizes.append(max([imageHeight,imageWidth]))\n",
    "            if iSet == 'Test':\n",
    "                testSpheroids.append(croppedImage)\n",
    "                if iClass == 'NonSphHealthy':\n",
    "                    testLabels.append([0])\n",
    "                if iClass == 'NonSphNonHealthy':\n",
    "                    testLabels.append([1])\n",
    "                if iClass == 'SphHealthy':\n",
    "                    testLabels.append([2])\n",
    "                if iClass == 'SphNonHealthy':\n",
    "                    testLabels.append([3])\n",
    "            if iSet == 'Training':\n",
    "                trainingSpheroids.append(croppedImage)\n",
    "                if iClass == 'NonSphHealthy':\n",
    "                    trainingLabels.append([0])\n",
    "                if iClass == 'NonSphNonHealthy':\n",
    "                    trainingLabels.append([1])\n",
    "                if iClass == 'SphHealthy':\n",
    "                    trainingLabels.append([2])\n",
    "                if iClass == 'SphNonHealthy':\n",
    "                    trainingLabels.append([3])\n",
    "            if iSet == 'Validation':\n",
    "                validationLabels.append(croppedImage)\n",
    "                if iClass == 'NonSphHealthy':\n",
    "                    validationLabels.append([0])\n",
    "                if iClass == 'NonSphNonHealthy':\n",
    "                    validationLabels.append([1])\n",
    "                if iClass == 'SphHealthy':\n",
    "                    validationLabels.append([2])\n",
    "                if iClass == 'SphNonHealthy':\n",
    "                    validationLabels.append([3])\n",
    "\n",
    "maximumImageSize =  max(allSizes)\n",
    "print(max(allSizes))          \n",
    "segmentationData = { \"testSpheroids\": testSpheroids, \"testLabels\": testLabels,\"trainingSpheroids\": testSpheroids, \"trainingLabels\": testLabels,\"validationSpheroids\": testSpheroids, \"validationLabels\": testLabels,\"maximumImageSize\": maximumImageSize}\n",
    "pickle.dump( segmentationData, open( \"segmentationData.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load segmentation DATA\n",
    "from generateBatch import generateBatch\n",
    "segmentationData = pickle.load( open( \"segmentationData.p\", \"rb\" ) )\n",
    "maximumImageSize = segmentationData[\"maximumImageSize\"]\n",
    "outputImageSize = 128\n",
    "numberOfReplicates = 10\n",
    "trainingSet = generateBatch(segmentationData[\"trainingSpheroids\"],segmentationData[\"trainingLabels\"], maximumImageSize, outputImageSize,numberOfReplicates)\n",
    "testingSet = generateBatch(segmentationData[\"testSpheroids\"],segmentationData[\"testLabels\"], maximumImageSize, outputImageSize,numberOfReplicates)\n",
    "validationSet = generateBatch(segmentationData[\"validationSpheroids\"],segmentationData[\"validationLabels\"], maximumImageSize, outputImageSize,numberOfReplicates)\n",
    "\n",
    "cnnData = {\"testingSet\": testingSet,\"trainingSet\": trainingSet,\"validationSet\": validationSet}\n",
    "pickle.dump( cnnData, open( \"cnnData.p\", \"wb\" ) )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "790\n"
     ]
    }
   ],
   "source": [
    "cnnData = pickle.load( open( \"cnnData.p\", \"rb\" ) )\n",
    "validationSet = cnnData[\"trainingSet\"]\n",
    "testingSet = cnnData[\"testingSet\"] \n",
    "trainingSet =  cnnData[\"validationSet\"]\n",
    "outputImageSize = 128\n",
    "numberOfClasses = 4\n",
    "print(len(trainingSet[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(<TensorType(float64, matrix)>, Elemwise{Cast{int32}}.0), (<TensorType(float64, matrix)>, Elemwise{Cast{int32}}.0), (<TensorType(float64, matrix)>, Elemwise{Cast{int32}}.0)]\n",
      "... building the model\n",
      "... training\n",
      "('training @ iter = ', 0L)\n",
      "epoch 1, minibatch 1/1, validation error 56.962025 %\n",
      "     epoch 1, minibatch 1/1, test error of best model 57.088608 %\n",
      "epoch 2, minibatch 1/1, validation error 54.430380 %\n",
      "     epoch 2, minibatch 1/1, test error of best model 54.430380 %\n",
      "epoch 3, minibatch 1/1, validation error 51.392405 %\n",
      "     epoch 3, minibatch 1/1, test error of best model 51.645570 %\n",
      "epoch 4, minibatch 1/1, validation error 46.835443 %\n",
      "     epoch 4, minibatch 1/1, test error of best model 46.835443 %\n",
      "epoch 5, minibatch 1/1, validation error 42.658228 %\n",
      "     epoch 5, minibatch 1/1, test error of best model 42.531646 %\n",
      "epoch 6, minibatch 1/1, validation error 38.354430 %\n",
      "     epoch 6, minibatch 1/1, test error of best model 38.987342 %\n"
     ]
    }
   ],
   "source": [
    "from loadCNNData import loadCNNData\n",
    "from CNNClasses import HiddenLayer,LogisticRegression,LeNetConvPoolLayer\n",
    "import os\n",
    "import sys\n",
    "import timeit\n",
    "\n",
    "import numpy\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.tensor.signal import pool\n",
    "from theano.tensor.nnet import conv2d\n",
    "\n",
    "\n",
    "def evaluate_lenet5(learning_rate=0.5, n_epochs=200,\n",
    "                    nkerns=[20, 50], batch_size=869, numberOfClasses=4):\n",
    "   \n",
    "\n",
    "    rng = numpy.random.RandomState(23455)\n",
    "\n",
    "    datasets = loadCNNData(trainingSet,testingSet,validationSet)\n",
    "\n",
    "    train_set_x, train_set_y = datasets[0]\n",
    "    valid_set_x, valid_set_y = datasets[1]\n",
    "    test_set_x, test_set_y = datasets[2]\n",
    "\n",
    "    # compute number of minibatches for training, validation and testing\n",
    "    n_train_batches = train_set_x.get_value(borrow=True).shape[0]\n",
    "    n_valid_batches = valid_set_x.get_value(borrow=True).shape[0]\n",
    "    n_test_batches = test_set_x.get_value(borrow=True).shape[0]\n",
    "    n_train_batches //= batch_size\n",
    "    n_valid_batches //= batch_size\n",
    "    n_test_batches //= batch_size\n",
    "\n",
    "    # allocate symbolic variables for the data\n",
    "    index = T.lscalar()  # index to a [mini]batch\n",
    "\n",
    "    # start-snippet-1\n",
    "    x = T.matrix('x')   # the data is presented as rasterized images\n",
    "    y = T.ivector('y')  # the labels are presented as 1D vector of\n",
    "                        # [int] labels\n",
    "\n",
    "    ######################\n",
    "    # BUILD ACTUAL MODEL #\n",
    "    ######################\n",
    "    print('... building the model')\n",
    "\n",
    "    # Reshape matrix of rasterized images of shape (batch_size, 128 * 128)\n",
    "    # to a 4D tensor, compatible with our LeNetConvPoolLayer\n",
    "    # (128, 128) is the size of images.\n",
    "    layer0_input = x.reshape((batch_size, 1, 128, 128))\n",
    "\n",
    "    # Construct the first convolutional pooling layer:\n",
    "    # filtering reduces the image size to (28-5+1 , 28-5+1) = (24, 24)\n",
    "    # maxpooling reduces this further to (24/2, 24/2) = (12, 12)\n",
    "    # 4D output tensor is thus of shape (batch_size, nkerns[0], 12, 12)\n",
    "    layer0 = LeNetConvPoolLayer(\n",
    "        rng,\n",
    "        input=layer0_input,\n",
    "        image_shape=(batch_size, 1, 128, 128),\n",
    "        filter_shape=(nkerns[0], 1, 23, 23),\n",
    "        poolsize=(2, 2)\n",
    "    )\n",
    "\n",
    "    # Construct the second convolutional pooling layer\n",
    "    # filtering reduces the image size to (12-5+1, 12-5+1) = (8, 8)\n",
    "    # maxpooling reduces this further to (8/2, 8/2) = (4, 4)\n",
    "    # 4D output tensor is thus of shape (batch_size, nkerns[1], 4, 4)\n",
    "    layer1 = LeNetConvPoolLayer(\n",
    "        rng,\n",
    "        input=layer0.output,\n",
    "        image_shape=(batch_size, nkerns[0], 53, 53),\n",
    "        filter_shape=(nkerns[1], nkerns[0], 16, 16),\n",
    "        poolsize=(2, 2)\n",
    "    )\n",
    "\n",
    "    # the HiddenLayer being fully-connected, it operates on 2D matrices of\n",
    "    # shape (batch_size, num_pixels) (i.e matrix of rasterized images).\n",
    "    # This will generate a matrix of shape (batch_size, nkerns[1] * 4 * 4),\n",
    "    # or (500, 50 * 4 * 4) = (500, 800) with the default values.\n",
    "    layer2_input = layer1.output.flatten(2)\n",
    "\n",
    "    # construct a fully-connected sigmoidal layer\n",
    "    layer2 = HiddenLayer(\n",
    "        rng,\n",
    "        input=layer2_input,\n",
    "        n_in=nkerns[1] * 19 * 19,\n",
    "        n_out=batch_size,\n",
    "        activation=T.tanh\n",
    "    )\n",
    "\n",
    "    # classify the values of the fully-connected sigmoidal layer\n",
    "    layer3 = LogisticRegression(input=layer2.output, n_in=batch_size, n_out=numberOfClasses)\n",
    "\n",
    "    # the cost we minimize during training is the NLL of the model\n",
    "    cost = layer3.negative_log_likelihood(y)\n",
    "\n",
    "    # create a function to compute the mistakes that are made by the model\n",
    "    test_model = theano.function(\n",
    "        [index],\n",
    "        layer3.errors(y),\n",
    "        givens={\n",
    "            x: test_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "            y: test_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    validate_model = theano.function(\n",
    "        [index],\n",
    "        layer3.errors(y),\n",
    "        givens={\n",
    "            x: valid_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "            y: valid_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # create a list of all model parameters to be fit by gradient descent\n",
    "    params = layer3.params + layer2.params + layer1.params + layer0.params\n",
    "\n",
    "    # create a list of gradients for all model parameters\n",
    "    grads = T.grad(cost, params)\n",
    "\n",
    "    # train_model is a function that updates the model parameters by\n",
    "    # SGD Since this model has many parameters, it would be tedious to\n",
    "    # manually create an update rule for each model parameter. We thus\n",
    "    # create the updates list by automatically looping over all\n",
    "    # (params[i], grads[i]) pairs.\n",
    "    updates = [\n",
    "        (param_i, param_i - learning_rate * grad_i)\n",
    "        for param_i, grad_i in zip(params, grads)\n",
    "    ]\n",
    "\n",
    "    train_model = theano.function(\n",
    "        [index],\n",
    "        cost,\n",
    "        updates=updates,\n",
    "        givens={\n",
    "            x: train_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "            y: train_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "    # end-snippet-1\n",
    "\n",
    "    ###############\n",
    "    # TRAIN MODEL #\n",
    "    ###############\n",
    "    print('... training')\n",
    "    # early-stopping parameters\n",
    "    patience = 10000  # look as this many examples regardless\n",
    "    patience_increase = 1  # wait this much longer when a new best is\n",
    "                           # found\n",
    "    improvement_threshold = 0.995  # a relative improvement of this much is\n",
    "                                   # considered significant\n",
    "    validation_frequency = min(n_train_batches, patience // 2)\n",
    "                                  # go through this many\n",
    "                                  # minibatche before checking the network\n",
    "                                  # on the validation set; in this case we\n",
    "                                  # check every epoch\n",
    "\n",
    "    best_validation_loss = numpy.inf\n",
    "    best_iter = 0\n",
    "    test_score = 0.\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    epoch = 0\n",
    "    done_looping = False\n",
    "\n",
    "    while (epoch < n_epochs) and (not done_looping):\n",
    "        epoch = epoch + 1\n",
    "        for minibatch_index in range(n_train_batches):\n",
    "\n",
    "            iter = (epoch - 1) * n_train_batches + minibatch_index\n",
    "\n",
    "            if iter % 100 == 0:\n",
    "                print('training @ iter = ', iter)\n",
    "            cost_ij = train_model(minibatch_index)\n",
    "\n",
    "            if (iter + 1) % validation_frequency == 0:\n",
    "\n",
    "                # compute zero-one loss on validation set\n",
    "                validation_losses = [validate_model(i) for i\n",
    "                                     in range(n_valid_batches)]\n",
    "                this_validation_loss = numpy.mean(validation_losses)\n",
    "                print('epoch %i, minibatch %i/%i, validation error %f %%' %\n",
    "                      (epoch, minibatch_index + 1, n_train_batches,\n",
    "                       this_validation_loss * 100.))\n",
    "\n",
    "                # if we got the best validation score until now\n",
    "                if this_validation_loss < best_validation_loss:\n",
    "\n",
    "                    #improve patience if loss improvement is good enough\n",
    "                    if this_validation_loss < best_validation_loss *  \\\n",
    "                       improvement_threshold:\n",
    "                        patience = max(patience, iter * patience_increase)\n",
    "\n",
    "                    # save best validation score and iteration number\n",
    "                    best_validation_loss = this_validation_loss\n",
    "                    best_iter = iter\n",
    "\n",
    "                    # test it on the test set\n",
    "                    test_losses = [\n",
    "                        test_model(i)\n",
    "                        for i in range(n_test_batches)\n",
    "                    ]\n",
    "                    test_score = numpy.mean(test_losses)\n",
    "                    print(('     epoch %i, minibatch %i/%i, test error of '\n",
    "                           'best model %f %%') %\n",
    "                          (epoch, minibatch_index + 1, n_train_batches,\n",
    "                           test_score * 100.))\n",
    "\n",
    "            if patience <= iter:\n",
    "                done_looping = True\n",
    "                break\n",
    "\n",
    "    end_time = timeit.default_timer()\n",
    "    print('Optimization complete.')\n",
    "    print('Best validation score of %f %% obtained at iteration %i, '\n",
    "          'with test performance %f %%' %\n",
    "          (best_validation_loss * 100., best_iter + 1, test_score * 100.))\n",
    "    print(('The code for file ' + ' ran for %.2fm' % ((end_time - start_time) / 60.)))\n",
    "\n",
    "\n",
    "evaluate_lenet5(learning_rate=0.5, n_epochs=200,nkerns=[20, 50], batch_size=790, numberOfClasses=4)\n",
    "\n",
    "def experiment(state, channel):\n",
    "    evaluate_lenet5(state.learning_rate, dataset=state.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from logisticRegression import LogisticRegression\n",
    "from loadCNNData import loadCNNData\n",
    "\n",
    "\n",
    "\n",
    "def sgd_optimization_mnist(learning_rate=0.2, n_epochs=2000,dataset='mnist.pkl.gz',batch_size=10,imageSize=32,numberOfClasses=4):\n",
    "    \"\"\"\n",
    "    Demonstrate stochastic gradient descent optimization of a log-linear\n",
    "    model\n",
    "\n",
    "    :type learning_rate: float\n",
    "    :param learning_rate: learning rate used (factor for the stochastic\n",
    "                          gradient)\n",
    "\n",
    "    :type n_epochs: int\n",
    "    :param n_epochs: maximal number of epochs to run the optimizer\n",
    "\n",
    "    :type dataset: string\n",
    "    :param dataset: the path of the MNIST dataset file from\n",
    "                 http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz\n",
    "\n",
    "    \"\"\"\n",
    "    datasets = loadCNNData(trainingSet,testingSet,validationSet)\n",
    "    #print(datasets[2])\n",
    "    \n",
    "    train_set_x, train_set_y = datasets[0]\n",
    "    valid_set_x, valid_set_y = datasets[1]\n",
    "    test_set_x, test_set_y = datasets[2]\n",
    "\n",
    "    # compute number of minibatches for training, validation and testing\n",
    "    n_train_batches = train_set_x.get_value(borrow=True).shape[0] // batch_size\n",
    "    n_valid_batches = valid_set_x.get_value(borrow=True).shape[0] // batch_size\n",
    "    n_test_batches = test_set_x.get_value(borrow=True).shape[0] // batch_size\n",
    "\n",
    "    ######################\n",
    "    # BUILD ACTUAL MODEL #\n",
    "    ######################\n",
    "    print('... building the model')\n",
    "\n",
    "    # allocate symbolic variables for the data\n",
    "    index = T.lscalar()  # index to a [mini]batch\n",
    "\n",
    "    # generate symbolic variables for input (x and y represent a\n",
    "    # minibatch)\n",
    "    x = T.matrix('x')  # data, presented as rasterized images\n",
    "    y = T.ivector('y')  # labels, presented as 1D vector of [int] labels\n",
    "\n",
    "    # construct the logistic regression class\n",
    "    # Each MNIST image has size 32*32\n",
    "    classifier = LogisticRegression(input=x, n_in=imageSize * imageSize, n_out=numberOfClasses)\n",
    "\n",
    "    # the cost we minimize during training is the negative log likelihood of\n",
    "    # the model in symbolic format\n",
    "    cost = classifier.negative_log_likelihood(y)\n",
    "\n",
    "    # compiling a Theano function that computes the mistakes that are made by\n",
    "    # the model on a minibatch\n",
    "    test_model = theano.function(\n",
    "        inputs=[index],\n",
    "        outputs=classifier.errors(y),\n",
    "        givens={\n",
    "            x: test_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "            y: test_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    validate_model = theano.function(\n",
    "        inputs=[index],\n",
    "        outputs=classifier.errors(y),\n",
    "        givens={\n",
    "            x: valid_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "            y: valid_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # compute the gradient of cost with respect to theta = (W,b)\n",
    "    g_W = T.grad(cost=cost, wrt=classifier.W)\n",
    "    g_b = T.grad(cost=cost, wrt=classifier.b)\n",
    "\n",
    "    # start-snippet-3\n",
    "    # specify how to update the parameters of the model as a list of\n",
    "    # (variable, update expression) pairs.\n",
    "    updates = [(classifier.W, classifier.W - learning_rate * g_W),\n",
    "               (classifier.b, classifier.b - learning_rate * g_b)]\n",
    "\n",
    "    # compiling a Theano function `train_model` that returns the cost, but in\n",
    "    # the same time updates the parameter of the model based on the rules\n",
    "    # defined in `updates`\n",
    "    train_model = theano.function(\n",
    "        inputs=[index],\n",
    "        outputs=cost,\n",
    "        updates=updates,\n",
    "        givens={\n",
    "            x: train_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "            y: train_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "    # end-snippet-3\n",
    "\n",
    "    ###############\n",
    "    # TRAIN MODEL #\n",
    "    ###############\n",
    "    print('... training the model')\n",
    "    # early-stopping parameters\n",
    "    patience = 3000  # look as this many examples regardless\n",
    "    patience_increase = 1  # wait this much longer when a new best is\n",
    "                                  # found\n",
    "    improvement_threshold = 0.995  # a relative improvement of this much is\n",
    "                                  # considered significant\n",
    "    validation_frequency = min(n_train_batches, patience // 2)\n",
    "                                  # go through this many\n",
    "                                  # minibatche before checking the network\n",
    "                                  # on the validation set; in this case we\n",
    "                                  # check every epoch\n",
    "\n",
    "    best_validation_loss = numpy.inf\n",
    "    test_score = 0.\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    done_looping = False\n",
    "    epoch = 0\n",
    "    while (epoch < n_epochs) and (not done_looping):\n",
    "        epoch = epoch + 1\n",
    "        for minibatch_index in range(n_train_batches):\n",
    "            minibatch_avg_cost = train_model(minibatch_index)\n",
    "            # iteration number\n",
    "            iter = (epoch - 1) * n_train_batches + minibatch_index\n",
    "\n",
    "            if (iter + 1) % validation_frequency == 0:\n",
    "                # compute zero-one loss on validation set\n",
    "                validation_losses = [validate_model(i)\n",
    "                                     for i in range(n_valid_batches)]\n",
    "                this_validation_loss = numpy.mean(validation_losses)\n",
    "\n",
    "                print(\n",
    "                    'epoch %i, minibatch %i/%i, validation error %f %%' %\n",
    "                    (\n",
    "                        epoch,\n",
    "                        minibatch_index + 1,\n",
    "                        n_train_batches,\n",
    "                        this_validation_loss * 100.\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                # if we got the best validation score until now\n",
    "                if this_validation_loss < best_validation_loss:\n",
    "                    #improve patience if loss improvement is good enough\n",
    "                    if this_validation_loss < best_validation_loss *  \\\n",
    "                       improvement_threshold:\n",
    "                        patience = max(patience, iter * patience_increase)\n",
    "\n",
    "                    best_validation_loss = this_validation_loss\n",
    "                    # test it on the test set\n",
    "\n",
    "                    test_losses = [test_model(i)\n",
    "                                   for i in range(n_test_batches)]\n",
    "                    test_score = numpy.mean(test_losses)\n",
    "\n",
    "                    print(\n",
    "                        (\n",
    "                            '     epoch %i, minibatch %i/%i, test error of'\n",
    "                            ' best model %f %%'\n",
    "                        ) %\n",
    "                        (\n",
    "                            epoch,\n",
    "                            minibatch_index + 1,\n",
    "                            n_train_batches,\n",
    "                            test_score * 100.\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                    # save the best model\n",
    "                    with open('best_model.pkl', 'wb') as f:\n",
    "                        pickle.dump(classifier, f)\n",
    "\n",
    "            if patience <= iter:\n",
    "                done_looping = True\n",
    "                break\n",
    "\n",
    "    end_time = timeit.default_timer()\n",
    "    print(('Optimization complete with best validation score of %f %%,' 'with test performance %f %%')% (best_validation_loss * 100., test_score * 100.))\n",
    "    #  print('The code run for %d epochs, with %f epochs/sec' % (epoch, 1. * epoch / (end_time - start_time)))\n",
    "    #print(('The code for file ' + os.path.split(__file__)[1] + ' ran for %.1fs' % ((end_time - start_time))), file=sys.stderr)\n",
    "\n",
    "\n",
    "def predict():\n",
    "    \"\"\"\n",
    "    An example of how to load a trained model and use it\n",
    "    to predict labels.\n",
    "    \"\"\"\n",
    "\n",
    "    # load the saved model\n",
    "    classifier = pickle.load(open('best_model.pkl'))\n",
    "\n",
    "    # compile a predictor function\n",
    "    predict_model = theano.function(\n",
    "        inputs=[classifier.input],\n",
    "        outputs=classifier.y_pred)\n",
    "\n",
    "    # We can test it on some examples from test test\n",
    "    \n",
    "    datasets = loadCNNData(trainingSet,testingSet,validationSet)\n",
    "    test_set_x, test_set_y = datasets[2]\n",
    "    test_set_x = test_set_x.get_value()\n",
    "\n",
    "    predicted_values = predict_model(test_set_x[:10])\n",
    "    print(\"Predicted values for the first 10 examples in test set:\")\n",
    "    print(predicted_values)\n",
    "\n",
    "\n",
    "\n",
    "sgd_optimization_mnist(learning_rate=0.9, n_epochs=1000,dataset='mnist.pkl.gz',batch_size=100,imageSize=outputImageSize,numberOfClasses=2)\n",
    "predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(testingSet[1])\n",
    "plt.imshow(segmentationData['testSpheroids'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This tutorial introduces the multilayer perceptron using Theano.\n",
    "\n",
    " A multilayer perceptron is a logistic regressor where\n",
    "instead of feeding the input to the logistic regression you insert a\n",
    "intermediate layer, called the hidden layer, that has a nonlinear\n",
    "activation function (usually tanh or sigmoid) . One can use many such\n",
    "hidden layers making the architecture deep. The tutorial will also tackle\n",
    "the problem of MNIST digit classification.\n",
    "\n",
    ".. math::\n",
    "\n",
    "    f(x) = G( b^{(2)} + W^{(2)}( s( b^{(1)} + W^{(1)} x))),\n",
    "\n",
    "References:\n",
    "\n",
    "    - textbooks: \"Pattern Recognition and Machine Learning\" -\n",
    "                 Christopher M. Bishop, section 5\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from logisticRegression import LogisticRegression\n",
    "from loadCNNData import loadCNNData\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import timeit\n",
    "\n",
    "import numpy\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "from hiddenLayer import HiddenLayer\n",
    "\n",
    "\n",
    "\n",
    "# start-snippet-2\n",
    "class MLP(object):\n",
    "    \"\"\"Multi-Layer Perceptron Class\n",
    "\n",
    "    A multilayer perceptron is a feedforward artificial neural network model\n",
    "    that has one layer or more of hidden units and nonlinear activations.\n",
    "    Intermediate layers usually have as activation function tanh or the\n",
    "    sigmoid function (defined here by a ``HiddenLayer`` class)  while the\n",
    "    top layer is a softmax layer (defined here by a ``LogisticRegression``\n",
    "    class).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, rng, input, n_in, n_hidden, n_out):\n",
    "        \"\"\"Initialize the parameters for the multilayer perceptron\n",
    "\n",
    "        :type rng: numpy.random.RandomState\n",
    "        :param rng: a random number generator used to initialize weights\n",
    "\n",
    "        :type input: theano.tensor.TensorType\n",
    "        :param input: symbolic variable that describes the input of the\n",
    "        architecture (one minibatch)\n",
    "\n",
    "        :type n_in: int\n",
    "        :param n_in: number of input units, the dimension of the space in\n",
    "        which the datapoints lie\n",
    "\n",
    "        :type n_hidden: int\n",
    "        :param n_hidden: number of hidden units\n",
    "\n",
    "        :type n_out: int\n",
    "        :param n_out: number of output units, the dimension of the space in\n",
    "        which the labels lie\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Since we are dealing with a one hidden layer MLP, this will translate\n",
    "        # into a HiddenLayer with a tanh activation function connected to the\n",
    "        # LogisticRegression layer; the activation function can be replaced by\n",
    "        # sigmoid or any other nonlinear function\n",
    "        self.hiddenLayer = HiddenLayer(\n",
    "            rng=rng,\n",
    "            input=input,\n",
    "            n_in=n_in,\n",
    "            n_out=n_hidden,\n",
    "            activation=T.tanh\n",
    "        )\n",
    "\n",
    "        # The logistic regression layer gets as input the hidden units\n",
    "        # of the hidden layer\n",
    "        self.logRegressionLayer = LogisticRegression(\n",
    "            input=self.hiddenLayer.output,\n",
    "            n_in=n_hidden,\n",
    "            n_out=n_out\n",
    "        )\n",
    "        # end-snippet-2 start-snippet-3\n",
    "        # L1 norm ; one regularization option is to enforce L1 norm to\n",
    "        # be small\n",
    "        self.L1 = (\n",
    "            abs(self.hiddenLayer.W).sum()\n",
    "            + abs(self.logRegressionLayer.W).sum()\n",
    "        )\n",
    "\n",
    "        # square of L2 norm ; one regularization option is to enforce\n",
    "        # square of L2 norm to be small\n",
    "        self.L2_sqr = (\n",
    "            (self.hiddenLayer.W ** 2).sum()\n",
    "            + (self.logRegressionLayer.W ** 2).sum()\n",
    "        )\n",
    "\n",
    "        # negative log likelihood of the MLP is given by the negative\n",
    "        # log likelihood of the output of the model, computed in the\n",
    "        # logistic regression layer\n",
    "        self.negative_log_likelihood = (\n",
    "            self.logRegressionLayer.negative_log_likelihood\n",
    "        )\n",
    "        # same holds for the function computing the number of errors\n",
    "        self.errors = self.logRegressionLayer.errors\n",
    "\n",
    "        # the parameters of the model are the parameters of the two layer it is\n",
    "        # made out of\n",
    "        self.params = self.hiddenLayer.params + self.logRegressionLayer.params\n",
    "        # end-snippet-3\n",
    "\n",
    "        # keep track of model input\n",
    "        self.input = input\n",
    "\n",
    "\n",
    "def test_mlp(learning_rate=0.9, L1_reg=0.00, L2_reg=0.0001, n_epochs=1000,\n",
    "             dataset='mnist.pkl.gz', batch_size=100, n_hidden=500):\n",
    "    \"\"\"\n",
    "    Demonstrate stochastic gradient descent optimization for a multilayer\n",
    "    perceptron\n",
    "\n",
    "    This is demonstrated on MNIST.\n",
    "\n",
    "    :type learning_rate: float\n",
    "    :param learning_rate: learning rate used (factor for the stochastic\n",
    "    gradient\n",
    "\n",
    "    :type L1_reg: float\n",
    "    :param L1_reg: L1-norm's weight when added to the cost (see\n",
    "    regularization)\n",
    "\n",
    "    :type L2_reg: float\n",
    "    :param L2_reg: L2-norm's weight when added to the cost (see\n",
    "    regularization)\n",
    "\n",
    "    :type n_epochs: int\n",
    "    :param n_epochs: maximal number of epochs to run the optimizer\n",
    "\n",
    "    :type dataset: string\n",
    "    :param dataset: the path of the MNIST dataset file from\n",
    "                 http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz\n",
    "\n",
    "\n",
    "   \"\"\"\n",
    "    datasets = loadCNNData(trainingSet,testingSet,validationSet)\n",
    "\n",
    "    train_set_x, train_set_y = datasets[0]\n",
    "    valid_set_x, valid_set_y = datasets[1]\n",
    "    test_set_x, test_set_y = datasets[2]\n",
    "\n",
    "    # compute number of minibatches for training, validation and testing\n",
    "    n_train_batches = train_set_x.get_value(borrow=True).shape[0] // batch_size\n",
    "    n_valid_batches = valid_set_x.get_value(borrow=True).shape[0] // batch_size\n",
    "    n_test_batches = test_set_x.get_value(borrow=True).shape[0] // batch_size\n",
    "\n",
    "    ######################\n",
    "    # BUILD ACTUAL MODEL #\n",
    "    ######################\n",
    "    print('... building the model')\n",
    "\n",
    "    # allocate symbolic variables for the data\n",
    "    index = T.lscalar()  # index to a [mini]batch\n",
    "    x = T.matrix('x')  # the data is presented as rasterized images\n",
    "    y = T.ivector('y')  # the labels are presented as 1D vector of\n",
    "                        # [int] labels\n",
    "\n",
    "    rng = numpy.random.RandomState(1234)\n",
    "\n",
    "    # construct the MLP class\n",
    "    classifier = MLP(\n",
    "        rng=rng,\n",
    "        input=x,\n",
    "        n_in=128 * 128,\n",
    "        n_hidden=n_hidden,\n",
    "        n_out=2\n",
    "    )\n",
    "\n",
    "    # start-snippet-4\n",
    "    # the cost we minimize during training is the negative log likelihood of\n",
    "    # the model plus the regularization terms (L1 and L2); cost is expressed\n",
    "    # here symbolically\n",
    "    cost = (\n",
    "        classifier.negative_log_likelihood(y)\n",
    "        + L1_reg * classifier.L1\n",
    "        + L2_reg * classifier.L2_sqr\n",
    "    )\n",
    "    # end-snippet-4\n",
    "\n",
    "    # compiling a Theano function that computes the mistakes that are made\n",
    "    # by the model on a minibatch\n",
    "    test_model = theano.function(\n",
    "        inputs=[index],\n",
    "        outputs=classifier.errors(y),\n",
    "        givens={\n",
    "            x: test_set_x[index * batch_size:(index + 1) * batch_size],\n",
    "            y: test_set_y[index * batch_size:(index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    validate_model = theano.function(\n",
    "        inputs=[index],\n",
    "        outputs=classifier.errors(y),\n",
    "        givens={\n",
    "            x: valid_set_x[index * batch_size:(index + 1) * batch_size],\n",
    "            y: valid_set_y[index * batch_size:(index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # start-snippet-5\n",
    "    # compute the gradient of cost with respect to theta (sorted in params)\n",
    "    # the resulting gradients will be stored in a list gparams\n",
    "    gparams = [T.grad(cost, param) for param in classifier.params]\n",
    "\n",
    "    # specify how to update the parameters of the model as a list of\n",
    "    # (variable, update expression) pairs\n",
    "\n",
    "    # given two lists of the same length, A = [a1, a2, a3, a4] and\n",
    "    # B = [b1, b2, b3, b4], zip generates a list C of same size, where each\n",
    "    # element is a pair formed from the two lists :\n",
    "    #    C = [(a1, b1), (a2, b2), (a3, b3), (a4, b4)]\n",
    "    updates = [\n",
    "        (param, param - learning_rate * gparam)\n",
    "        for param, gparam in zip(classifier.params, gparams)\n",
    "    ]\n",
    "\n",
    "    # compiling a Theano function `train_model` that returns the cost, but\n",
    "    # in the same time updates the parameter of the model based on the rules\n",
    "    # defined in `updates`\n",
    "    train_model = theano.function(\n",
    "        inputs=[index],\n",
    "        outputs=cost,\n",
    "        updates=updates,\n",
    "        givens={\n",
    "            x: train_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "            y: train_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "    # end-snippet-5\n",
    "\n",
    "    ###############\n",
    "    # TRAIN MODEL #\n",
    "    ###############\n",
    "    print('... training')\n",
    "\n",
    "    # early-stopping parameters\n",
    "    patience = 1000  # look as this many examples regardless\n",
    "    patience_increase = 2  # wait this much longer when a new best is\n",
    "                           # found\n",
    "    improvement_threshold = 0.995  # a relative improvement of this much is\n",
    "                                   # considered significant\n",
    "    validation_frequency = min(n_train_batches, patience // 2)\n",
    "                                  # go through this many\n",
    "                                  # minibatche before checking the network\n",
    "                                  # on the validation set; in this case we\n",
    "                                  # check every epoch\n",
    "\n",
    "    best_validation_loss = numpy.inf\n",
    "    best_iter = 0\n",
    "    test_score = 0.\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    epoch = 0\n",
    "    done_looping = False\n",
    "\n",
    "    while (epoch < n_epochs) and (not done_looping):\n",
    "        epoch = epoch + 1\n",
    "        for minibatch_index in range(n_train_batches):\n",
    "\n",
    "            minibatch_avg_cost = train_model(minibatch_index)\n",
    "            # iteration number\n",
    "            iter = (epoch - 1) * n_train_batches + minibatch_index\n",
    "\n",
    "            if (iter + 1) % validation_frequency == 0:\n",
    "                # compute zero-one loss on validation set\n",
    "                validation_losses = [validate_model(i) for i\n",
    "                                     in range(n_valid_batches)]\n",
    "                this_validation_loss = numpy.mean(validation_losses)\n",
    "\n",
    "                print(\n",
    "                    'epoch %i, minibatch %i/%i, validation error %f %%' %\n",
    "                    (\n",
    "                        epoch,\n",
    "                        minibatch_index + 1,\n",
    "                        n_train_batches,\n",
    "                        this_validation_loss * 100.\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                # if we got the best validation score until now\n",
    "                if this_validation_loss < best_validation_loss:\n",
    "                    #improve patience if loss improvement is good enough\n",
    "                    if (\n",
    "                        this_validation_loss < best_validation_loss *\n",
    "                        improvement_threshold\n",
    "                    ):\n",
    "                        patience = max(patience, iter * patience_increase)\n",
    "\n",
    "                    best_validation_loss = this_validation_loss\n",
    "                    best_iter = iter\n",
    "\n",
    "                    # test it on the test set\n",
    "                    test_losses = [test_model(i) for i\n",
    "                                   in range(n_test_batches)]\n",
    "                    test_score = numpy.mean(test_losses)\n",
    "\n",
    "                    print(('     epoch %i, minibatch %i/%i, test error of '\n",
    "                           'best model %f %%') %\n",
    "                          (epoch, minibatch_index + 1, n_train_batches,\n",
    "                           test_score * 100.))\n",
    "\n",
    "            if patience <= iter:\n",
    "                done_looping = True\n",
    "                break\n",
    "\n",
    "    end_time = timeit.default_timer()\n",
    "    print(('Optimization complete. Best validation score of %f %% '\n",
    "           'obtained at iteration %i, with test performance %f %%') %\n",
    "          (best_validation_loss * 100., best_iter + 1, test_score * 100.))\n",
    "    print('The code for file ' +\n",
    "           'MLP' +\n",
    "           ' ran for %.2fm' % ((end_time - start_time) / 60.))\n",
    "\n",
    "\n",
    "\n",
    "test_mlp()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
